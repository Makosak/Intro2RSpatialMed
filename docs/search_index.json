[["index.html", "Intro to R-Spatial for Healthy Places Background", " Intro to R-Spatial for Healthy Places 2023-06-13 Background This suite of tutorials was developed for a workshop at the 2021 R-Medicine Conference by the Healthy Regions &amp; Policies Lab at the University of Illinois. It has sinced been updated for multiple workshops at the Society for Epidemiology Research Annual Meeting. This workbook is a quick (3-hour) overview of mapping, GIScience, and spatial analysis basics for health professionals. The workbook was compiled by Marynia Kolak, co-facilitated with Qinyun Lin, with past support from Susan Paykin when HEROP was at the University of Chicago. Some coding snippets &amp; data examples are from the phenomenal team of the Opioid Environment Toolkit (Moksha Menghaney, Qinyun Lin, Angela Li). The overall approach follows the Center for Spatial Data Science UChicago paradigm, led by Luc Anselin &amp; Julia Koschinsky. Environment Setup A basic understanding of R is assumed. This workshop requires several packages, which can be installed from CRAN: install.packages(&quot;sf&quot;, &quot;tmap&quot;, &quot;tidygeocoder&quot;) For Mac users, check out https://github.com/r-spatial/sf for additional tips if you run into errors when installing the sf package. Using homebrew to install gdal usually fixes any remaining issues. "],["01-intro.html", "1 Intro to Spatial Data 1.1 Load Spatial Data 1.2 Non-Spatial &amp; Spatial Views 1.3 Spatial Data Structure 1.4 Exploring Coordinate Reference Systems 1.5 Refine Basic Map 1.6 Arrange multiple maps 1.7 Interactive Mode 1.8 Overlay Zip Code Boundaries More Resources", " 1 Intro to Spatial Data In the workshop, we learned about: What is Spatial Data? What is the sf framework for R? To delve in further, let’s see some spatial data in action. We’ll work with the sf library first. library(sf) 1.1 Load Spatial Data First load in the shapefile. Remember, this type of data is actually comprised of multiple files. All need to be present in order to read correctly. Chi_tracts = st_read(&quot;data/geo_export_aae47441-adab-4aca-8cb0-2e0c0114096e.shp&quot;) ## Reading layer `geo_export_aae47441-adab-4aca-8cb0-2e0c0114096e&#39; from data source ## `/Users/maryniakolak/Code/Intro2RSpatialMed/data/geo_export_aae47441-adab-4aca-8cb0-2e0c0114096e.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 801 features and 9 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -87.94025 ymin: 41.64429 xmax: -87.52366 ymax: 42.02392 ## Geodetic CRS: WGS84(DD) 1.2 Non-Spatial &amp; Spatial Views Always inspect data when loading in. First we look at a non-spatial view. head(Chi_tracts) ## Simple feature collection with 6 features and 9 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -87.68822 ymin: 41.72902 xmax: -87.62394 ymax: 41.87455 ## Geodetic CRS: WGS84(DD) ## commarea commarea_n countyfp10 geoid10 name10 namelsad10 notes statefp10 tractce10 ## 1 44 44 031 17031842400 8424 Census Tract 8424 &lt;NA&gt; 17 842400 ## 2 59 59 031 17031840300 8403 Census Tract 8403 &lt;NA&gt; 17 840300 ## 3 34 34 031 17031841100 8411 Census Tract 8411 &lt;NA&gt; 17 841100 ## 4 31 31 031 17031841200 8412 Census Tract 8412 &lt;NA&gt; 17 841200 ## 5 32 32 031 17031839000 8390 Census Tract 8390 &lt;NA&gt; 17 839000 ## 6 28 28 031 17031838200 8382 Census Tract 8382 &lt;NA&gt; 17 838200 ## geometry ## 1 POLYGON ((-87.62405 41.7302... ## 2 POLYGON ((-87.68608 41.8229... ## 3 POLYGON ((-87.62935 41.8528... ## 4 POLYGON ((-87.68813 41.8556... ## 5 POLYGON ((-87.63312 41.8744... ## 6 POLYGON ((-87.66782 41.8741... Note the last column – this is a spatially enabled column. The data is no longer a ‘shapefile’ but an `sf’ object, comprised of polygons. We can use a baseR function to view the spatial dimension. The sf framework enables previews of each attribute in our spatial file. plot(Chi_tracts) 1.3 Spatial Data Structure Check out the data structure of this file… What object is it? str(Chi_tracts) ## Classes &#39;sf&#39; and &#39;data.frame&#39;: 801 obs. of 10 variables: ## $ commarea : chr &quot;44&quot; &quot;59&quot; &quot;34&quot; &quot;31&quot; ... ## $ commarea_n: num 44 59 34 31 32 28 65 53 76 77 ... ## $ countyfp10: chr &quot;031&quot; &quot;031&quot; &quot;031&quot; &quot;031&quot; ... ## $ geoid10 : chr &quot;17031842400&quot; &quot;17031840300&quot; &quot;17031841100&quot; &quot;17031841200&quot; ... ## $ name10 : chr &quot;8424&quot; &quot;8403&quot; &quot;8411&quot; &quot;8412&quot; ... ## $ namelsad10: chr &quot;Census Tract 8424&quot; &quot;Census Tract 8403&quot; &quot;Census Tract 8411&quot; &quot;Census Tract 8412&quot; ... ## $ notes : chr NA NA NA NA ... ## $ statefp10 : chr &quot;17&quot; &quot;17&quot; &quot;17&quot; &quot;17&quot; ... ## $ tractce10 : chr &quot;842400&quot; &quot;840300&quot; &quot;841100&quot; &quot;841200&quot; ... ## $ geometry :sfc_POLYGON of length 801; first list element: List of 1 ## ..$ : num [1:243, 1:2] -87.6 -87.6 -87.6 -87.6 -87.6 ... ## ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;XY&quot; &quot;POLYGON&quot; &quot;sfg&quot; ## - attr(*, &quot;sf_column&quot;)= chr &quot;geometry&quot; ## - attr(*, &quot;agr&quot;)= Factor w/ 3 levels &quot;constant&quot;,&quot;aggregate&quot;,..: NA NA NA NA NA NA NA NA NA ## ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;commarea&quot; &quot;commarea_n&quot; &quot;countyfp10&quot; &quot;geoid10&quot; ... Check out the coordinate reference system. What is it? What are the units? st_crs(Chi_tracts) ## Coordinate Reference System: ## User input: WGS84(DD) ## wkt: ## GEOGCRS[&quot;WGS84(DD)&quot;, ## DATUM[&quot;WGS84&quot;, ## ELLIPSOID[&quot;WGS84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1, ## ID[&quot;EPSG&quot;,9001]]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic longitude&quot;,east, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic latitude&quot;,north, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]]] 1.4 Exploring Coordinate Reference Systems Lets see how switching CRS changes our object. First we’ll try the Mollweide coordinate reference system that does a good job preserving area across the globe. To transform our CRS, we use the st_transform function. To plot, we use baseR again but with some paremeter updates. Finally, we check out the CRS of our new object. What are the units? Any other details to note? Will this be appropriate for our spatial analysis? Chi_tracts.moll &lt;- st_transform(Chi_tracts, crs=&quot;ESRI:54009&quot;) plot(st_geometry(Chi_tracts.moll), border = &quot;gray&quot;, lwd = 2, main = &quot;Mollweide&quot;, sub=&quot;preserves areas&quot;) st_crs(Chi_tracts.moll) ## Coordinate Reference System: ## User input: ESRI:54009 ## wkt: ## PROJCRS[&quot;World_Mollweide&quot;, ## BASEGEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]]], ## CONVERSION[&quot;World_Mollweide&quot;, ## METHOD[&quot;Mollweide&quot;], ## PARAMETER[&quot;Longitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;False easting&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Not known.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;ESRI&quot;,54009]] Next, we’ll try the Winkel CRS, which is a compromise projection that facilitates minimal distortion for area, distance, and angles. We use the same approach, recyling the code with new inputs. Chi_tracts.54019 = st_transform(Chi_tracts, crs=&quot;ESRI:54019&quot;) plot(st_geometry(Chi_tracts.54019), border = &quot;gray&quot;, lwd = 2, main = &quot;Winkel&quot;, sub=&quot;minimal distortion&quot;) st_crs(Chi_tracts.54019) ## Coordinate Reference System: ## User input: ESRI:54019 ## wkt: ## PROJCRS[&quot;World_Winkel_II&quot;, ## BASEGEOGCRS[&quot;WGS 84&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]]], ## CONVERSION[&quot;World_Winkel_II&quot;, ## METHOD[&quot;Winkel II&quot;], ## PARAMETER[&quot;Longitude of natural origin&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Latitude of 1st standard parallel&quot;,50.4597762521898, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8823]], ## PARAMETER[&quot;False easting&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Not known.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;ESRI&quot;,54019]] We could also try a totally different projection, to see how that changes our spatial object. Let’s use the “Old Hawaiian UTM Zone 4n” projection, with the EPSG identified from an online search. How does this fare? Chi_tracts.Hawaii = st_transform(Chi_tracts, crs=&quot;ESRI:102114&quot;) plot(st_geometry(Chi_tracts.Hawaii), border = &quot;gray&quot;, lwd = 2, main = &quot;Old Hawaiian UTM Zone 4N&quot;, sub=&quot;wrong projection!&quot;) Finally.. let’s choose a projection that is focused on Illinois, and uses distance as feet or meters, to make it a bit more accessible for our work. EPSG:3435 is a good fit: Chi_tracts.3435 &lt;- st_transform(Chi_tracts, &quot;EPSG:3435&quot;) # Chi_tracts.3435 &lt;- st_transform(Chi_tracts, 3435) st_crs(Chi_tracts.3435) ## Coordinate Reference System: ## User input: EPSG:3435 ## wkt: ## PROJCRS[&quot;NAD83 / Illinois East (ftUS)&quot;, ## BASEGEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4269]], ## CONVERSION[&quot;SPCS83 Illinois East zone (US Survey feet)&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,36.6666666666667, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,-88.3333333333333, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.999975, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,984250, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,0, ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;easting (X)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## AXIS[&quot;northing (Y)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;US survey foot&quot;,0.304800609601219]], ## USAGE[ ## SCOPE[&quot;Engineering survey, topographic mapping.&quot;], ## AREA[&quot;United States (USA) - Illinois - counties of Boone; Champaign; Clark; Clay; Coles; Cook; Crawford; Cumberland; De Kalb; De Witt; Douglas; Du Page; Edgar; Edwards; Effingham; Fayette; Ford; Franklin; Gallatin; Grundy; Hamilton; Hardin; Iroquois; Jasper; Jefferson; Johnson; Kane; Kankakee; Kendall; La Salle; Lake; Lawrence; Livingston; Macon; Marion; Massac; McHenry; McLean; Moultrie; Piatt; Pope; Richland; Saline; Shelby; Vermilion; Wabash; Wayne; White; Will; Williamson.&quot;], ## BBOX[37.06,-89.28,42.5,-87.02]], ## ID[&quot;EPSG&quot;,3435]] plot(st_geometry(Chi_tracts.3435), border = &quot;gray&quot;, lwd = 2, main = &quot;NAD83 / Illinois East (ftUS)&quot;, sub=&quot;topo mapping &amp; survey use&quot;) 1.5 Refine Basic Map Now we’ll switch to a more extensive cartographic mapping package, tmap. We approach mapping with one layer at a time. Always start with the object you want to map by calling it with the tm_shape function. Then, at least one descriptive/styling function follows. There are hundreds of variations and paramater specifications, so take your time in exploring tmap and the options. Here we style the tracts with some semi-transparent borders. library(tmap) tm_shape(Chi_tracts) + tm_borders(alpha=0.5) Next we fill the tracts with a light gray, and adjust the color and transparency of borders. We also add a scale bar, positioning it to the left and having a thickness of 0.8 units, and turn off the frame. tm_shape(Chi_tracts) + tm_fill(col = &quot;gray90&quot;) + tm_borders(alpha=0.2, col = &quot;gray10&quot;) + tm_scale_bar(position = (&quot;left&quot;), lwd = 0.8) + tm_layout(frame = F) Check out https://rdrr.io/cran/tmap/man/tm_polygons.html for more ideas! 1.6 Arrange multiple maps Sometimes we want to look at multiple maps at once. Write your mapping function to a new variable, and then call that variable in order of desire using the tmap_arrange function. Hint: this is just one of many! ways to map multiples using tmap… see if you can uncover more in the documentation. tracts.4326 &lt;- tm_shape(Chi_tracts) + tm_fill(col = &quot;gray90&quot;) + tm_layout(frame = F, title = &quot;EPSG 4326&quot;) tracts.54019 &lt;- tm_shape(Chi_tracts.54019) + tm_fill(col = &quot;gray90&quot;) + tm_layout(frame = F, title = &quot;EPSG 54019&quot;) tmap_arrange(tracts.4326, tracts.54019) 1.7 Interactive Mode So far, we’ve been plotting static maps. We can also switch to an interactive map that uses a Leaflet widget by switching the tmap_mode() parameter specification from “plot” to “view.” It’s on “plot” as default. tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing Map the same map as before, and check out the interaction! tm_shape(Chi_tracts) + tm_fill(col = &quot;gray90&quot;) + tm_borders(alpha=0.2, col = &quot;gray10&quot;) + tm_scale_bar(position = (&quot;left&quot;), lwd = 0.8) + tm_layout(frame = F) The tracts are not transparent enough, so we update that here. You can also click the box on the left side to try out other basemaps. See if you can find out how to add a basemap to a static/plotted map, using tmap documentation… tm_shape(Chi_tracts) + tm_fill(col = &quot;gray90&quot;, alpha = 0.5) + tm_borders(alpha=0.2, col = &quot;gray10&quot;) + tm_scale_bar(position = (&quot;left&quot;), lwd = 0.8) + tm_layout(frame = F) We revert back to plot mode for now. tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting 1.8 Overlay Zip Code Boundaries How do census tract areas correspond to zip codes? While tracts better represent neighborhoods, often times we are stuck with zip code level scale in healh research. Here we’ll make a reference map to highlight tract distribution across each zip code. First, we read in zip code boundaries. This data was downloaded directly from the City of Chicago Data Portal as a shapefile. Chi_Zips = st_read(&quot;data/geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5.shp&quot;) ## Reading layer `geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5&#39; from data source ## `/Users/maryniakolak/Code/Intro2RSpatialMed/data/geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 61 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.94011 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304 ## Geodetic CRS: WGS84(DD) Next, we layer the new shape in – on top of the tracts. We use a thicker border, and try out a new color. Experiment! ## FIRST LAYER: CENSUS TRACT BOUNADRIES tm_shape(Chi_tracts.3435) + tm_fill(col = &quot;gray90&quot;) + tm_borders(alpha=0.2, col = &quot;gray10&quot;) + ## SECOND LAYER: ZIP CODE BOUNDARIES WITH LABEL tm_shape(Chi_Zips) + tm_borders(lwd = 2, col = &quot;#0099CC&quot;) + tm_text(&quot;zip&quot;, size = 0.7) + ## MORE CARTOGRAPHIC STYLE tm_scale_bar(position = (&quot;left&quot;), lwd = 0.8) + tm_layout(frame = F) More Resources On spatial data basics &amp; sf: https://geocompr.robinlovelace.net/intro.html https://geodacenter.github.io/opioid-environment-toolkit/spatial-data-introduction.html On projections: https://desktop.arcgis.com/en/arcmap/10.3/guide-books/map-projections/projection-basics-for-gis-professionals.htm https://geocompr.robinlovelace.net/reproj-geo-data.html https://datacarpentry.org/organization-geospatial/03-crs/index.html On tmap: https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html https://geocompr.robinlovelace.net/adv-map.html "],["02-choropleth.html", "2 Map Neighborhoods 2.1 Clean Attribute Data 2.2 Merge Spatial Data 2.3 Quantile Maps 2.4 Standard Deviation Maps 2.5 Jenks Maps 2.6 Integrate More Data 2.7 Thematic Map Panel 2.8 Data More Resources", " 2 Map Neighborhoods When considering the health of persons, we have to also consider the neighborhood environment. Sometimes this is looking at neighborhood level health outcomes, like premature mortality at the census tract scale, or cumulative COVID rates by zip code. Sometimes we’re interested in neighborhood factors like poverty, access to affordable housing, or distance to nearest health provider, or pollution-emitting facility. These measurements of the “social determinants of health” at the neighborhood scale are increasingly urgent in modern public health thinking, and are thought to drive and/or reinforce racial, social, and spatial inequity In this module, we’ll learn about the basics of thematic mapping – known as choropleth mapping – to visualize neighborhood level health phenomena. This will allow you to begin the process of exploratory spatial data analysis and hypothesis generation &amp; refinement. 2.1 Clean Attribute Data Let’s consider COVID-19 cases by zip code in Chicago. We’ll upload and inspect a summary of cases from the Chicago Data Portal first: COVID &lt;- read.csv(&quot;data/COVID-19_Cases__Tests__and_Deaths_by_ZIP_Code.csv&quot;) head(COVID) ## ZIP.Code Week.Number Week.Start Week.End Cases...Weekly Cases...Cumulative Case.Rate...Weekly ## 1 60603 39 09/20/2020 09/26/2020 0 13 0 ## 2 60604 39 09/20/2020 09/26/2020 0 31 0 ## 3 60611 16 04/12/2020 04/18/2020 8 72 25 ## 4 60611 15 04/05/2020 04/11/2020 7 64 22 ## 5 60615 11 03/08/2020 03/14/2020 NA NA NA ## 6 60603 10 03/01/2020 03/07/2020 NA NA NA ## Case.Rate...Cumulative Tests...Weekly Tests...Cumulative Test.Rate...Weekly Test.Rate...Cumulative ## 1 1107.3 25 327 2130 27853.5 ## 2 3964.2 12 339 1534 43350.4 ## 3 222.0 101 450 312 1387.8 ## 4 197.4 59 349 182 1076.3 ## 5 NA 6 9 14 21.7 ## 6 NA 0 0 0 0.0 ## Percent.Tested.Positive...Weekly Percent.Tested.Positive...Cumulative Deaths...Weekly Deaths...Cumulative ## 1 0.0 0.0 0 0 ## 2 0.0 0.1 0 0 ## 3 0.1 0.2 0 0 ## 4 0.1 0.2 0 0 ## 5 NA NA 0 0 ## 6 NA NA 0 0 ## Death.Rate...Weekly Death.Rate...Cumulative Population Row.ID ZIP.Code.Location ## 1 0 0 1174 60603-39 POINT (-87.625473 41.880112) ## 2 0 0 782 60604-39 POINT (-87.629029 41.878153) ## 3 0 0 32426 60611-16 POINT (-87.620291 41.894734) ## 4 0 0 32426 60611-15 POINT (-87.620291 41.894734) ## 5 0 0 41563 60615-11 POINT (-87.602725 41.801993) ## 6 0 0 1174 60603-10 POINT (-87.625473 41.880112) Each row corresponds to a zip code at a different week. This data thus exists as a “long” format, which doesn’t work for spatial analysis. We need to convert to “wide” format, or at the very least, ensure that each zip code corresponds to one row. To simplify, let’s identify the last week of the dataset, and then subset the data frame to only show that week. We will be interested in the cumulative case rate. Following is one way of doing this – can you think of another way? Try out different approaches of reshaping data to test your R and “tidy” skills. ## How many weeks are in our dataset? range(as.numeric(COVID$Week.Number)) ## [1] 10 40 ## Subset &amp; inspect to week 39. COVID.39 &lt;- subset(COVID, COVID$Week.Number == &quot;39&quot;) head(COVID.39) ## ZIP.Code Week.Number Week.Start Week.End Cases...Weekly Cases...Cumulative Case.Rate...Weekly ## 1 60603 39 09/20/2020 09/26/2020 0 13 0 ## 2 60604 39 09/20/2020 09/26/2020 0 31 0 ## 36 60601 39 09/20/2020 09/26/2020 8 213 54 ## 37 60602 39 09/20/2020 09/26/2020 0 21 0 ## 41 60605 39 09/20/2020 09/26/2020 12 391 44 ## 66 60610 39 09/20/2020 09/26/2020 35 666 90 ## Case.Rate...Cumulative Tests...Weekly Tests...Cumulative Test.Rate...Weekly Test.Rate...Cumulative ## 1 1107.3 25 327 2130 27853.5 ## 2 3964.2 12 339 1534 43350.4 ## 36 1451.4 202 4304 1376 29328.8 ## 37 1688.1 27 460 2170 36977.5 ## 41 1420.8 291 7160 1058 26018.4 ## 66 1706.9 500 10680 1281 27371.3 ## Percent.Tested.Positive...Weekly Percent.Tested.Positive...Cumulative Deaths...Weekly Deaths...Cumulative ## 1 0.0 0.0 0 0 ## 2 0.0 0.1 0 0 ## 36 0.0 0.0 1 6 ## 37 0.0 0.0 0 0 ## 41 0.0 0.1 1 3 ## 66 0.1 0.1 0 10 ## Death.Rate...Weekly Death.Rate...Cumulative Population Row.ID ZIP.Code.Location ## 1 0.0 0.0 1174 60603-39 POINT (-87.625473 41.880112) ## 2 0.0 0.0 782 60604-39 POINT (-87.629029 41.878153) ## 36 6.8 40.9 14675 60601-39 POINT (-87.622844 41.886262) ## 37 0.0 0.0 1244 60602-39 POINT (-87.628309 41.883136) ## 41 3.6 10.9 27519 60605-39 POINT (-87.623449 41.867824) ## 66 0.0 25.6 39019 60610-39 POINT (-87.63581 41.90455) To clean our data a bit, we’ll just keep the zip code name, and cumulative case rate for the week of September 20th, 2020. COVID.39f &lt;- COVID.39[,c(&quot;ZIP.Code&quot;, &quot;Case.Rate...Cumulative&quot;)] head(COVID.39f) ## ZIP.Code Case.Rate...Cumulative ## 1 60603 1107.3 ## 2 60604 3964.2 ## 36 60601 1451.4 ## 37 60602 1688.1 ## 41 60605 1420.8 ## 66 60610 1706.9 2.2 Merge Spatial Data Next, let’s merge this data to our zip code master spatial file. Reload if necessary: library(sf) Chi_Zips = st_read(&quot;data/geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5.shp&quot;) ## Reading layer `geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5&#39; from data source ## `/Users/maryniakolak/Code/Intro2RSpatialMed/data/geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 61 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.94011 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304 ## Geodetic CRS: WGS84(DD) head(Chi_Zips) ## Simple feature collection with 6 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.80649 ymin: 41.88747 xmax: -87.59852 ymax: 41.93228 ## Geodetic CRS: WGS84(DD) ## objectid shape_area shape_len zip geometry ## 1 33 106052287 42720.04 60647 MULTIPOLYGON (((-87.67762 4... ## 2 34 127476051 48103.78 60639 MULTIPOLYGON (((-87.72683 4... ## 3 35 45069038 27288.61 60707 MULTIPOLYGON (((-87.785 41.... ## 4 36 70853834 42527.99 60622 MULTIPOLYGON (((-87.66707 4... ## 5 37 99039621 47970.14 60651 MULTIPOLYGON (((-87.70656 4... ## 6 38 23506056 34689.35 60611 MULTIPOLYGON (((-87.61401 4... Next, merge on zip code ID. The key in the Chi_Zips object is zip, whereas the key for the COVID data is ZIP.code. Always merge non-spatial to spatial data, not the other way around. Think of the spatial file as your master file that you will continue to add on to… #Chi_Zipsf &lt;- merge(Chi_Zips, COVID.39f, by.x = &quot;zip&quot;, by.y = &quot;ZIP.Code&quot;, all = TRUE) Chi_Zipsf &lt;- merge(Chi_Zips, COVID.39f, by.x = &quot;zip&quot;, by.y = &quot;ZIP.Code&quot;) head(Chi_Zipsf) ## Simple feature collection with 6 features and 5 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.63999 ymin: 41.85317 xmax: -87.60246 ymax: 41.88913 ## Geodetic CRS: WGS84(DD) ## zip objectid shape_area shape_len Case.Rate...Cumulative geometry ## 1 60601 27 9166246 19804.58 1451.4 MULTIPOLYGON (((-87.62271 4... ## 2 60602 26 4847125 14448.17 1688.1 MULTIPOLYGON (((-87.60997 4... ## 3 60603 19 4560229 13672.68 1107.3 MULTIPOLYGON (((-87.61633 4... ## 4 60604 48 4294902 12245.81 3964.2 MULTIPOLYGON (((-87.63376 4... ## 5 60605 20 36301276 37973.35 1420.8 MULTIPOLYGON (((-87.62064 4... ## 6 60606 31 6766411 12040.44 2289.6 MULTIPOLYGON (((-87.63397 4... 2.3 Quantile Maps Starting with a “classic epi” approach, let’s look at case rates as quantiles. We use the tmap library, and update the choropleth data classification using the style parameter. We use the Blue-Purple palette, or BuPu, from Colorbrewer. Colorbrewer Tip: To display all Colorbrewer palette options, load the RColorBrewer library and run display.brewer.all() – or just Google “R Colorbrewer palettes.” library(tmap) tmap_mode(&quot;plot&quot;) tm_shape(Chi_Zipsf ) + tm_polygons(&quot;Case.Rate...Cumulative&quot;, style=&quot;quantile&quot;, pal=&quot;BuPu&quot;, title = &quot;COVID Case Rate&quot;) Let’s try tertiles: tm_shape(Chi_Zipsf ) + tm_polygons(&quot;Case.Rate...Cumulative&quot;, style=&quot;quantile&quot;, n=3, pal=&quot;BuPu&quot;, title = &quot;COVID Case Rate&quot;) 2.4 Standard Deviation Maps While quantiles are a nice start, let’s classify using a standard deviation map. Standard deviation is a statistical technique type of map based on how much the data differs from the mean. tm_shape(Chi_Zipsf ) + tm_polygons(&quot;Case.Rate...Cumulative&quot;, style=&quot;sd&quot;, pal=&quot;BuPu&quot;, title = &quot;COVID Case Rate&quot;) 2.5 Jenks Maps Another approach of data classification is natural breaks, or jenks. This approach looks for “natural breaks” in the data using a univariate clustering algorithm. tm_shape(Chi_Zipsf ) + tm_polygons(&quot;Case.Rate...Cumulative&quot;, style=&quot;jenks&quot;, pal=&quot;BuPu&quot;, title = &quot;COVID Case Rate&quot;) The first bin doesn’t seem very intuitive. Let’s try 4 bins instead of 5 by changing the n parameter. In this version, we’ll also had a histogram and scale bar, and move the legend outside the frame to make it easier to view. tm_shape(Chi_Zipsf) + tm_polygons(&quot;Case.Rate...Cumulative&quot;, style=&quot;jenks&quot;, pal=&quot;BuPu&quot;, legend.hist=T, n=4, title = &quot;COVID Case Rate&quot;, ) + tm_scale_bar(position = &quot;left&quot;) + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) 2.6 Integrate More Data To explore potential disparities in COVID health outcomes, let’s bring in pre-cleaned demographic, racial, and ethnic data from the Opioid Environment Policy Scan database. This data is orginally sourced from the American Community Survey 2018 5-year estimate, which you could also pull using the tidycensus. CensusVar &lt;- read.csv(&quot;data/DS01_Z.csv&quot;) head(CensusVar) ## ZCTA year totPopE whiteP blackP amIndP asianP pacIsP otherP hispP noHSP age0_4 age5_14 age15_19 age20_24 ## 1 35004 2018 11762 84.39 13.09 0.00 0.94 0.00 1.57 0.94 5.52 787 1950 457 746 ## 2 35005 2018 7528 55.22 42.44 0.64 0.00 0.15 1.55 1.37 17.48 511 1055 455 277 ## 3 35006 2018 2927 96.04 3.21 0.27 0.00 0.00 0.48 0.00 14.44 161 413 141 203 ## 4 35007 2018 26328 73.83 13.75 0.04 1.33 0.02 11.01 11.11 12.41 1891 4161 1619 1400 ## 5 35010 2018 20625 63.07 32.43 0.39 0.65 0.00 3.45 4.10 22.00 1013 2647 1383 1087 ## 6 35013 2018 40 100.00 0.00 0.00 0.00 0.00 0.00 100.00 100.00 0 0 0 0 ## age15_44 age45_49 age50_54 age55_59 age60_64 ageOv65 ageOv18 age18_64 a15_24P und45P ovr65P disbP ## 1 4552 662 541 776 832 1662 8820 7158 10.23 61.97 14.13 12.7 ## 2 2429 580 469 560 552 1372 5691 4319 9.72 53.07 18.23 23.2 ## 3 878 129 193 316 278 559 2308 1749 11.75 49.61 19.10 20.9 ## 4 9947 1993 2067 1713 1315 3241 19178 15937 11.47 60.77 12.31 13.5 ## 5 7036 1418 1545 1510 1341 4115 16142 12027 11.98 51.86 19.95 19.6 ## 6 13 8 19 0 0 0 40 40 0.00 32.50 0.00 0.0 Merge to our master Zip Code dataset. Chi_Zipsf &lt;- merge(Chi_Zipsf, CensusVar, by.x = &quot;zip&quot;, by.y = &quot;ZCTA&quot;) head(Chi_Zipsf) ## Simple feature collection with 6 features and 31 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.63999 ymin: 41.85317 xmax: -87.60246 ymax: 41.88913 ## Geodetic CRS: WGS84(DD) ## zip objectid shape_area shape_len Case.Rate...Cumulative year totPopE whiteP blackP amIndP asianP pacIsP ## 1 60601 27 9166246 19804.58 1451.4 2018 14675 74.17 5.57 0.45 18.00 0.00 ## 2 60602 26 4847125 14448.17 1688.1 2018 1244 68.17 3.78 5.31 19.45 0.00 ## 3 60603 19 4560229 13672.68 1107.3 2018 1174 63.46 3.24 0.00 27.60 0.00 ## 4 60604 48 4294902 12245.81 3964.2 2018 782 63.43 5.63 0.00 29.67 0.00 ## 5 60605 20 36301276 37973.35 1420.8 2018 27519 61.20 17.18 0.18 16.10 0.03 ## 6 60606 31 6766411 12040.44 2289.6 2018 3101 72.75 2.35 0.00 18.09 0.00 ## otherP hispP noHSP age0_4 age5_14 age15_19 age20_24 age15_44 age45_49 age50_54 age55_59 age60_64 ageOv65 ## 1 1.81 8.68 0.00 550 156 907 909 8726 976 1009 324 859 2075 ## 2 3.30 6.51 0.00 61 87 18 91 987 46 53 0 5 5 ## 3 5.71 9.80 0.00 13 43 179 172 684 75 47 150 50 112 ## 4 1.28 4.35 0.00 12 7 52 168 450 27 47 54 92 93 ## 5 5.31 5.84 2.39 837 1279 2172 2282 16364 1766 1520 1824 1360 2569 ## 6 6.80 6.29 0.73 57 44 0 139 1863 213 153 168 172 431 ## ageOv18 age18_64 a15_24P und45P ovr65P disbP geometry ## 1 13855 11780 12.37 64.27 14.14 6.4 MULTIPOLYGON (((-87.62271 4... ## 2 1095 1090 8.76 91.24 0.40 0.2 MULTIPOLYGON (((-87.60997 4... ## 3 1118 1006 29.90 63.03 9.54 7.3 MULTIPOLYGON (((-87.61633 4... ## 4 744 651 28.13 59.97 11.89 4.1 MULTIPOLYGON (((-87.63376 4... ## 5 25259 22690 16.19 67.15 9.34 5.3 MULTIPOLYGON (((-87.62064 4... ## 6 3000 2569 4.48 63.33 13.90 1.9 MULTIPOLYGON (((-87.63397 4... 2.7 Thematic Map Panel To facilitate data discovery, we likely want to explore multiple maps at once. Here we’ll generate maps for multiple variables, and plot them as a map panel. Can you think of more efficient ways to run this code? There are also other tmap tricks to optimize this further, so enjoy your journey! tm_shape(Chi_Zipsf) + tm_fill(&quot;ovr65P&quot;, style=&quot;jenks&quot;, pal=&quot;BuPu&quot;, n=4) COVID &lt;- tm_shape(Chi_Zipsf) + tm_fill(&quot;Case.Rate...Cumulative&quot;, style=&quot;jenks&quot;, pal=&quot;Reds&quot;, n=4, title = &quot;COVID Rt&quot;) Senior &lt;- tm_shape(Chi_Zipsf) + tm_fill(&quot;ovr65P&quot;, style=&quot;jenks&quot;, pal=&quot;BuPu&quot;, n=4) NoHS &lt;- tm_shape(Chi_Zipsf) + tm_fill(&quot;noHSP&quot;, style=&quot;jenks&quot;, pal=&quot;BuPu&quot;, n=4) BlkP &lt;- tm_shape(Chi_Zipsf) + tm_fill(&quot;blackP&quot;, style=&quot;jenks&quot;, pal=&quot;BuPu&quot;, n=4) Latnx &lt;- tm_shape(Chi_Zipsf) + tm_fill(&quot;hispP&quot;, style=&quot;jenks&quot;, pal=&quot;BuPu&quot;, n=4, title=&quot;Test&quot;) WhiP &lt;- tm_shape(Chi_Zipsf) + tm_fill(&quot;whiteP&quot;, style=&quot;jenks&quot;, pal=&quot;BuPu&quot;, n=4) tmap_arrange(COVID, Senior, NoHS, BlkP, Latnx, WhiP) From the results, we see that cumulative COVID outcomes for one week in September 2020 seemed to have some geographic correlation with the Latinx/Hispanic community in Chicago. At the same time, low high school diploma rates are also concentrated in these areas, and there is some intersection with other variables considered. What are additional variables you could bring in to refine your approach? Perhaps percentage of essential workers; a different age group; internet access? What about linking in health outcomes like Asthma, Hypertension, and more at a similar scale? In modern spatial epidemiology, associations must never be taken at face value. For example, we know that it is not “race” but “racism” that drives multiple health disparities – simply looking at a specific racial/ethnic group is not enough. Thus exploring multiple variables and nurturing a curiosity to understand these complex intersections will support knowledge discovery. 2.8 Data We’re done! Well… not so fast. Let’s save the data so we don’t have to run the codebook again to access the data. Here, we’ll save as a geojson file. This spatial format is more forgiving with long column names, which is a long-standing challenge with shapefiles. But sometimes it can be written oddly, so double-check the dimensions when reading in later. #st_(Chi_Zipsf, &quot;data/ChiZipMaster.geojson&quot;, driver = &quot;GeoJSON&quot;) We could also just the data as a CSV file which may be easier for future linking. For this, we use the st_drop_geometry() function to remove the geometry data, so we’re just left with the attributes. #.csv(st_drop_geometry(Chi_Zipsf), &quot;data/ChiZipMaster.csv&quot;) More Resources For choropleth mapping in R: https://spatialanalysis.github.io/lab_tutorials/4_R_Mapping.html "],["05-LISA.html", "3 Spatial Cluster Detection 3.1 Identify Pattern 3.2 Load rGeoda 3.3 Define W 3.4 Calculate LISA 3.5 Map the LISA 3.6 Certainty Check 3.7 Putting it together More Resources", " 3 Spatial Cluster Detection Exploratory Spatial Data Analysis requires us to review the variable of interest multiple ways, with different methods, to detect patterns and uncover interesting trends. However, our minds are wired to see patterns, whether or not they are (statistically) there. In this chapter, we’ll test the COVID regional pattern we identified previously for statistically significant spatial clustering (or outlier) behavior. Our null hypothesis is spatial randomness; if the LISA (local indicator of spatial autocorrelation) for an area is high and statistically significant, we’ve identified a “hot spot” spatial cluster. (In other words, that area and it’s neighbors have higher rates of COVID cases, when compared to a spatially random map.) If the area has a low and statistically significant finding, it’s also a spatial cluster, but a cold spot. We can also detect spatial outliers, as discussed in the workshop. How we define neighbors will influence our findings. 3.1 Identify Pattern Let’s look at the most stable choropleth map from the last exercise. If you took a break, you’ll need to reload your two main libraries, sf and tmap for spatial data wrangling and detection. Try to practice spatial I/O by loading in your merged Zip-Code level dataset. library(sf) library(tmap) Chi_Zipsf &lt;- st_read(&quot;data/ChiZipMaster.geojson&quot;) ## Reading layer `ChiZipMaster&#39; from data source ## `/Users/maryniakolak/Code/Intro2RSpatialMed/data/ChiZipMaster.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 1080 features and 31 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.87596 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304 ## Geodetic CRS: WGS 84 After inspecting your dataset again, map your variable of interest, Cumulative Covid Case Rate, developed previously. # head(Chi_Zipsf) tm_shape(Chi_Zipsf) + tm_polygons(&quot;Case.Rate...Cumulative&quot;, style=&quot;jenks&quot;, pal=&quot;BuPu&quot;, legend.hist=T, n=4, title = &quot;COVID Case Rate&quot;, ) + tm_scale_bar(position = &quot;left&quot;) + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) We found that multuple areas on the West Side of Chicago persistently emerged as having higher case rates for this week of interest in our dataset. Is this a statistically significant finding, though? 3.2 Load rGeoda Next, we’ll load a brand new library,rgeoda. This library wraps core functions of GeoDa, an open source spatial statitical software developed by Luc Anselin’s team. Lead developer Xun Li brought over key functions to packages for various platforms, including R. Install the package, if you haven’t done so already, and load. For more details and troubleshooting tips, check out the documentation at https://geodacenter.github.io/rgeoda/. # install.packages(&quot;rgeoda) library(rgeoda) 3.3 Define W Next, we’ll generate multiple spatial weightws for comparison and hypothesis generation. A more local spatial neighbor weight, like rook, will assume less interaction/influence across space. When we create a queen contiguity weight with 2 orders, meaning influence may be up to 2 neighbors away, we specify that lower orders are also included. w.rook &lt;- rook_weights(Chi_Zipsf) w.queen &lt;- queen_weights(Chi_Zipsf) w.queen2 &lt;- queen_weights(Chi_Zipsf, order = 2, include_lower_order = TRUE) 3.4 Calculate LISA Next, we calculate our Local Moran’s I (LISA). The default assumes 999 permutations; we can try 499 permutations for one case to compare results lisa.rook &lt;- local_moran(w.rook, Chi_Zipsf[&#39;Case.Rate...Cumulative&#39;], permutations = 999) lisa.queen2 &lt;- local_moran(w.queen2, Chi_Zipsf[&#39;Case.Rate...Cumulative&#39;], permutations = 999) lisa.queen2.449 &lt;- local_moran(w.queen2, Chi_Zipsf[&#39;Case.Rate...Cumulative&#39;], permutations = 449) 3.5 Map the LISA Here, we extract sample code directly from the library tutorial to visualize LISAs. Explore the output on your own to determine how you might 1) attach LISA values or clusters to the master file, and 2) visualize using a different library lisa_colors.rook &lt;- lisa_colors(lisa.rook) lisa_labels.rook &lt;- lisa_labels(lisa.rook) lisa_clusters.rook &lt;- lisa_clusters(lisa.rook) plot(st_geometry(Chi_Zipsf), col=sapply(lisa_clusters.rook, function(x){return(lisa_colors.rook[[x+1]])}), border = &quot;#333333&quot;, lwd=0.2) title(main = &quot;Covid Case Rt LISA (Rook)&quot;) legend(&#39;bottomleft&#39;, legend = lisa_labels.rook, fill = lisa_colors.rook, border = &quot;#eeeeee&quot;) Next, we visualize with a 2nd order queen contiguity spatial weight. lisa_colors.queen &lt;- lisa_colors(lisa.queen2) lisa_labels.queen &lt;- lisa_labels(lisa.queen2) lisa_clusters.queen &lt;- lisa_clusters(lisa.queen2) plot(st_geometry(Chi_Zipsf), col=sapply(lisa_clusters.queen, function(x){return(lisa_colors.queen[[x+1]])}), border = &quot;#333333&quot;, lwd=0.2) title(main = &quot;Covid Case Rt LISA (Queen)&quot;) legend(&#39;bottomleft&#39;, legend = lisa_labels.queen, fill = lisa_colors.queen) Finally, we visualize with a 2nd order queen contiguity spatial weight using fewer permutations. lisa_colors.queen.499 &lt;- lisa_colors(lisa.queen2.449) lisa_labels.queen.499 &lt;- lisa_labels(lisa.queen2.449) lisa_clusters.queen.499 &lt;- lisa_clusters(lisa.queen2.449) plot(st_geometry(Chi_Zipsf), col=sapply(lisa_clusters.queen.499, function(x){return(lisa_colors.queen.499[[x+1]])}), border = &quot;#333333&quot;, lwd=0.2) title(main = &quot;Covid Case Rt LISA (Queen, 499)&quot;) legend(&#39;bottomleft&#39;, legend = lisa_labels.queen.499, fill = lisa_colors.queen.499) In this case, using fewer permutations did not change the output, suggesting a stable &amp; robust result. It is best practice to use more permutations when conducting a LISA analysis, though exploration is welcome and encourages. 3.6 Certainty Check How certain are we that areas identified are persistently clusters or outliers? We can view the significance level of each area to check our certainty – or, uncertainty. lisa_p &lt;- lisa_pvalues(lisa.queen2) p_labels &lt;- c(&quot;Not significant&quot;, &quot;p &lt;= 0.05&quot;, &quot;p &lt;= 0.01&quot;, &quot;p &lt;= 0.001&quot;) p_colors &lt;- c(&quot;#eeeeee&quot;, &quot;#84f576&quot;, &quot;#53c53c&quot;, &quot;#348124&quot;) plot(st_geometry(Chi_Zipsf), col=sapply(lisa_p, function(x){ if (x &lt;= 0.001) return(p_colors[4]) else if (x &lt;= 0.01) return(p_colors[3]) else if (x &lt;= 0.05) return (p_colors[2]) else return(p_colors[1]) }), border = &quot;#333333&quot;, lwd=0.2) title(main = &quot;Covid Case Rt LISA (Queen, P-Vlaue)&quot;) legend(&#39;bottomleft&#39;, legend = p_labels, fill = p_colors, border = &quot;#eeeeee&quot;) 3.7 Putting it together There was a slight change in the “hot spot” cluster when using a rook or 2nd order queen contiguity weight. Explore resource below to look for analytically-driven tips on how to determine an “optimal” setting. More importantly, understanding the phenomenon at hand is crucial for identifying a spatial weight. What are the underlying behaviors, phenomena, and spatial structures you are trying to examine? How would a spatial weight change for measuring exposure of an infectious disease, versus capturing spatially heterogeneous phenomena like redlining that also leave a spatial footprint? Would you use the same spatial weight, or different? More Resources https://geodacenter.github.io/rgeoda/ https://geodacenter.github.io/rgeoda/articles/rgeoda_tutorial.html https://geodacenter.github.io/workbook/5a_global_auto/lab5a.html#morans-i https://geodacenter.github.io/workbook/6a_local_auto/lab6a.html "],["03-overlaypoints.html", "4 Adding Resources 4.1 Geocode 4.2 Convert to Spatial Data 4.3 Basic Map of Points 4.4 Overlay Points &amp; Style 4.5 Integrate More Data 4.6 Graduated Symbology 4.7 Style Final Map", " 4 Adding Resources In addition to areal data, we can also extract information from individual locations. Locations, when measured as points, can include things like: Health providers: Hospitals, Clinics, Pharmacies, Mental health providers, Medication for opioid use disorder providers Area resources: Grocery stores &amp; Supermarkets, Playgrounds, Daycare centers, Schools, Community centers Area challenges: Crime, Superfund sites, Pollution-emitting facilities Points can also represent people, like individual patients residing in an area. Because individual locations for persons is protected health information, we’ll focus on point data as resources in the chapter. However, you can reuse the code snippets in this workshop to wrangle patient-level data the same way in a secure environment, under the guidance of your friendly IRB ethics board. In this example, we’ll extend our Chicago example. We’ll identify areas with high COVID rates, low geographic access to methadone maintenance therapy, and less access to affordable rental housing units managed by the city. We are interested in locating zip codes that may be especially vulnerable to persons with opioid use disorder who use MOUDs. (This is oversimplified, but our example to work with.) 4.1 Geocode If you start with only addresses, you’ll need to geocode. Our methadone maintenance provider dataset is only available as such. Addresses are comprised of characeters that reference a specific place. We will use the network topology service of a Geocoder to translate that address to a coordinate in some CRS. First we load the tidygeocoder to get our geocoding done. Note, this uses the interent to process, so is not suitable for HIPPA protected data like individual, living person addresses. For offline geocoders, check out Pelias or ESRI. library(tidygeocoder) Let’s read in and inspect data for methadone maintenance providers. Note, these addresses were made available by SAMSHA, and are known as publicly available information. An additional analysis could call each service to check on access to medication during COVID in Septmber 2020, and the list would be updated further. methadoneClinics &lt;- read.csv(&quot;data/chicago_methadone_nogeometry.csv&quot;) head(methadoneClinics) ## X Name Address City State Zip ## 1 1 Chicago Treatment and Counseling Center, Inc. 4453 North Broadway st. Chicago IL 60640 ## 2 2 Sundace Methadone Treatment Center, LLC 4545 North Broadway St. Chicago IL 60640 ## 3 3 Soft Landing Interventions/DBA Symetria Recovery of Lakeview 3934 N. Lincoln Ave. Chicago IL 60613 ## 4 4 PDSSC - Chicago, Inc. 2260 N. Elston Ave. Chicago IL 60614 ## 5 5 Center for Addictive Problems, Inc. 609 N. Wells St. Chicago IL 60654 ## 6 6 Family Guidance Centers, Inc. 310 W. Chicago Ave. Chicago IL 60654 Let’s geocode one address first, just to make sure our system is working. We’ll use the “cascade” method which use the US Census and OpenStreetMap geocoders. These two services are the main options with tidygeocoder. sample &lt;- geo(&quot;2260 N. Elston Ave. Chicago, IL&quot;, lat = latitude, long = longitude, method = &#39;cascade&#39;) ## Passing 1 address to the US Census single address geocoder ## Query completed in: 0.2 seconds head(sample) ## # A tibble: 1 × 4 ## address latitude longitude geo_method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2260 N. Elston Ave. Chicago, IL 41.9 -87.7 census As we prepare for geocoding, check out the structure of the dataset. Do we need to change anything? The data should be a character to be read properly. str(methadoneClinics) ## &#39;data.frame&#39;: 27 obs. of 6 variables: ## $ X : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Name : chr &quot;Chicago Treatment and Counseling Center, Inc.&quot; &quot;Sundace Methadone Treatment Center, LLC&quot; &quot;Soft Landing Interventions/DBA Symetria Recovery of Lakeview&quot; &quot;PDSSC - Chicago, Inc.&quot; ... ## $ Address: chr &quot;4453 North Broadway st.&quot; &quot;4545 North Broadway St.&quot; &quot;3934 N. Lincoln Ave.&quot; &quot;2260 N. Elston Ave.&quot; ... ## $ City : chr &quot;Chicago&quot; &quot;Chicago&quot; &quot;Chicago&quot; &quot;Chicago&quot; ... ## $ State : chr &quot;IL&quot; &quot;IL&quot; &quot;IL&quot; &quot;IL&quot; ... ## $ Zip : int 60640 60640 60613 60614 60654 60654 60651 60607 60607 60616 ... We need to clean the data a bit. We’ll add a new column for a full address, as required by the geocoding service. When you use a geocoding service, be sure to read the documentation and understand how the data needs to be formatted for input. methadoneClinics$fullAdd &lt;- paste(as.character(methadoneClinics$Address), as.character(methadoneClinics$City), as.character(methadoneClinics$State), as.character(methadoneClinics$Zip)) We’re ready to go! Batch geocode with one function, and inspect: geoCodedClinics &lt;- geocode(methadoneClinics, address = &#39;fullAdd&#39;, lat = latitude, long = longitude, method = &#39;cascade&#39;) ## Passing 27 addresses to the US Census batch geocoder ## Query completed in: 6.1 seconds ## Passing 1 address to the Nominatim single address geocoder ## Query completed in: 1 seconds head(geoCodedClinics) ## # A tibble: 6 × 10 ## X Name Address City State Zip fullAdd latitude longitude geo_method ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 Chicago Treatment and Counseling Cent… 4453 N… Chic… IL 60640 4453 N… 42.0 -87.7 census ## 2 2 Sundace Methadone Treatment Center, L… 4545 N… Chic… IL 60640 4545 N… 42.0 -87.7 census ## 3 3 Soft Landing Interventions/DBA Symetr… 3934 N… Chic… IL 60613 3934 N… 42.0 -87.7 census ## 4 4 PDSSC - Chicago, Inc. 2260 N… Chic… IL 60614 2260 N… 41.9 -87.7 census ## 5 5 Center for Addictive Problems, Inc. 609 N.… Chic… IL 60654 609 N.… 41.9 -87.6 census ## 6 6 Family Guidance Centers, Inc. 310 W.… Chic… IL 60654 310 W.… 41.9 -87.6 census There were two that didn’t geocode correctly. You can inspect further. This could involve a quick check for spelling issues; or, searching the address and pulling the lat/long using Google Maps and inputting manually. Or, if we are concerned it’s a human or unknown error, we could omit. For this exercise we’ll just omit the two clinics that didn’t geocode correctly. geoCodedClinics2 &lt;- na.omit(geoCodedClinics) 4.2 Convert to Spatial Data This is not spatial data yet! To convert a static file to spatial data, we use the powerful st_as_sf function from sf. Indicate the x,y parameters (=longitude, latitude) and the coordinate reference system used. Our geocoding service used the standard EPSG:4326, so we input that here. library(sf) methadoneSf &lt;- st_as_sf(geoCodedClinics2, coords = c( &quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) 4.3 Basic Map of Points For a really simple map of points – to ensure they were geocoded and converted to spatial data correctly, we use tmap. We’ll use the interactive version to view. library(tmap) tmap_mode(&quot;view&quot;) tm_shape(methadoneSf) + tm_dots() If your points didn’t plot correctly: Did you flip the longitude/latitude values? Did you input the correct CRS? Those two issues are the most common errors. 4.4 Overlay Points &amp; Style Let’s add our zip code map from the previous module. First load the data, then overlay. Chi_Zipsf &lt;- st_read(&quot;data/ChiZipMaster1.geojson&quot;) ## Reading layer `ChiZipMaster1&#39; from data source ## `/Users/maryniakolak/Code/Intro2RSpatialMed/data/ChiZipMaster1.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 540 features and 31 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.87596 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304 ## Geodetic CRS: WGS 84 With this overlay, we’ll add a “hack” to include the methadone clinic points in a legend. tmap_mode(&quot;plot&quot;) ## tmap mode set to plotting ## 1st layer (gets plotted first) tm_shape(Chi_Zipsf) + tm_fill(&quot;Case.Rate...Cumulative&quot;, style=&quot;jenks&quot;, pal=&quot;BuPu&quot;, n=4, title = &quot;COVID Rt&quot;) + ## 2nd layer (overlay) tm_shape(methadoneSf) + tm_dots(size = 0.2, col = &quot;gray20&quot;) + ## &quot;Hack&quot; a manual symbology for dots in the legend tm_add_legend(&quot;symbol&quot;, col = &quot;gray20&quot;, size = .2, labels = &quot;Methadone MOUD&quot;) + ## Cartographic Styling tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) 4.5 Integrate More Data From here, we can integrate more data. Let’s try a different point dataset – Affordable Rental Housing Developments, as made available by the City of Chicago Data Portal. This could be interesting for a number of different reasons – maybe we hypothesize better outcomes are associated with better access to affordable housing options? Or, we hypothesize the opposite, that mean distance to more population dense housing locations is vulnerable to airborne disease? For this example, we’ll think about this dataset as access to secure and affordable housing. Persons with lower incomes residing in places with fewer developments may be more vulnerable to housing insecurity -&gt; impacts health. AffHousing &lt;- read.csv(&quot;data/Affordable_Rental_Housing_Developments.csv&quot;) head(AffHousing) ## Community.Area.Name Community.Area.Number Property.Type Property.Name ## 1 Englewood 68 Veterans Hope Manor Village ## 2 Rogers Park 1 Senior Morse Senior Apts. ## 3 Uptown 3 ARO The Draper ## 4 Edgewater 77 Senior Pomeroy Apts. ## 5 Roseland 49 Supportive Housing Wentworth Commons ## 6 Humboldt Park 23 Multifamily Nelson Mandela Apts. ## Address Zip.Code Phone.Number Management.Company Units X.Coordinate ## 1 5900-6100 S. Green/Peoria/Sangamon 60621 312-564-2393 Volunteers of America Illinois 36 NA ## 2 6928 N. Wayne Ave. 60626 312-602-6207 Morse Urban Dev. 44 1165844 ## 3 5050 N. Broadway 60640 312-818-1722 Flats LLC 35 1167357 ## 4 5650 N. Kenmore Ave. 60660 773-275-7820 Habitat Company 198 1168181 ## 5 11045 S. Wentworth Ave. 60628 773-568-7804 Mercy Housing Lakefront 50 1176951 ## 6 607 N. Sawyer Ave. 60624 773-227-6332 Bickerdike Apts. 6 1154640 ## Y.Coordinate Latitude Longitude Location ## 1 NA NA NA ## 2 1946059 42.00757 -87.66517 (42.0075737709331, -87.6651711448293) ## 3 1933882 41.97413 -87.65996 (41.9741295261027, -87.6599553011627) ## 4 1937918 41.98519 -87.65681 (41.9851867755403, -87.656808676983) ## 5 1831516 41.69302 -87.62777 (41.6930159120977, -87.6277673462214) ## 6 1903912 41.89215 -87.70753 (41.8921534052465, -87.7075265659001) There were a few data points with odd inputs and null values. Remember, we can’t convert any null values to spatial coordinates. Again, in an ideal context, you would explore and understand what is happening, systematically. In our experiment, we’ll omit nulls. AffHousing &lt;- na.omit(AffHousing) Look at the structure of the object. str(AffHousing) ## &#39;data.frame&#39;: 487 obs. of 14 variables: ## $ Community.Area.Name : chr &quot;Rogers Park&quot; &quot;Uptown&quot; &quot;Edgewater&quot; &quot;Roseland&quot; ... ## $ Community.Area.Number: int 1 3 77 49 23 38 42 36 36 8 ... ## $ Property.Type : chr &quot;Senior&quot; &quot;ARO&quot; &quot;Senior&quot; &quot;Supportive Housing&quot; ... ## $ Property.Name : chr &quot;Morse Senior Apts.&quot; &quot;The Draper&quot; &quot;Pomeroy Apts.&quot; &quot;Wentworth Commons&quot; ... ## $ Address : chr &quot;6928 N. Wayne Ave.&quot; &quot;5050 N. Broadway&quot; &quot;5650 N. Kenmore Ave.&quot; &quot;11045 S. Wentworth Ave.&quot; ... ## $ Zip.Code : int 60626 60640 60660 60628 60624 60653 60637 60653 60653 60622 ... ## $ Phone.Number : chr &quot;312-602-6207&quot; &quot;312-818-1722&quot; &quot;773-275-7820&quot; &quot;773-568-7804&quot; ... ## $ Management.Company : chr &quot;Morse Urban Dev.&quot; &quot;Flats LLC&quot; &quot;Habitat Company&quot; &quot;Mercy Housing Lakefront&quot; ... ## $ Units : int 44 35 198 50 6 71 67 534 148 40 ... ## $ X.Coordinate : num 1165844 1167357 1168181 1176951 1154640 ... ## $ Y.Coordinate : num 1946059 1933882 1937918 1831516 1903912 ... ## $ Latitude : num 42 42 42 41.7 41.9 ... ## $ Longitude : num -87.7 -87.7 -87.7 -87.6 -87.7 ... ## $ Location : chr &quot;(42.0075737709331, -87.6651711448293)&quot; &quot;(41.9741295261027, -87.6599553011627)&quot; &quot;(41.9851867755403, -87.656808676983)&quot; &quot;(41.6930159120977, -87.6277673462214)&quot; ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int 1 ## ..- attr(*, &quot;names&quot;)= chr &quot;1&quot; In this dataset, we can see coordinate information is already included – twice! You’re looking at 2 different types of coordinate systems. We’ll use “Longitude” and “Latitude” to represent X,Y and an ESPG of 4326. We’re guessing, and hopeful. AffHousingSf &lt;- st_as_sf(AffHousing, coords = c(&quot;Longitude&quot;, &quot;Latitude&quot;), crs = 4326) We can now map the data for a quick view – does this look like Chicago, hopefully? tm_shape(AffHousingSf) + tm_bubbles(&quot;Units&quot;, col = &quot;purple&quot;, style = &quot;sd&quot;) 4.6 Graduated Symbology Previously we mapped points as dots. We literally used the tm_dots() function to do so. Another option is changing the size of the point, according to some attribute of the data. In this dataset, we see an attribute field that gives us the total number of units per housing site. Let’s use a graduated symbology, with the tm_bubbles() function, to map these points. That way points with more units will be bigger, and not all places are weighted the same visually. tm_shape(Chi_Zipsf) + tm_polygons(col = &quot;gray80&quot;) + tm_shape(AffHousingSf) + tm_bubbles(&quot;Units&quot;, col = &quot;purple&quot;) 4.7 Style Final Map Let’s pull what we learned in the last tutorial, and map everything at once. Which zip codes are the most vulnerable to persons with OUD during the pandemic in September 2020, based on the information we have here? ## Zip Codes with Labels tm_shape(Chi_Zipsf) + tm_fill(&quot;Case.Rate...Cumulative&quot;, style=&quot;jenks&quot;, pal=&quot;BuPu&quot;, n=4, title = &quot;COVID Rt&quot;) + ## Affordable Housing Units tm_shape(AffHousingSf) + tm_bubbles(&quot;Units&quot;) + ## Methadone MOUD tm_shape(methadoneSf) + tm_dots(size = 0.2, col = &quot;gray20&quot;) + ## Cartographic Styling tm_add_legend(&quot;symbol&quot;, col = &quot;gray20&quot;, size = .2, labels = &quot;Methadone MOUD&quot;) + tm_layout(legend.outside = TRUE, legend.outside.position = &quot;right&quot;) In RStudio, you could zoom into the plot you created to get a better view. Save as an image, or save as a webpage! Save any data you need from this session. st_write(methadoneSf, dsn = &quot;data/methadoneMOUD.geojson&quot;) "],["04-newsvar.html", "5 Calculate Spatial Metrics 5.1 Load Spatial Data 5.2 Transform Projections 5.3 Count resources by area 5.4 Buffer Data 5.5 Count buffers by area 5.6 Integrate &amp; Explore", " 5 Calculate Spatial Metrics While we’ve generated some nice visualizations, we need insights quantified as metrics at the neighborhood level. Don’t start this step until you have a good idea of what you need. Visualizing and exploring the data in depth is best practice. For our purposes, we’re interested in developing spatial access metrics with a container method approach. At the end of this tutorial, we’ll generate the following new variables: Total number of Methadone Maintenance MOUD by zip code Total number of Walkble MOUD Service Areas by zip code Plus, we will have a new spatial layer, that includes the actual service areas (ie. 1-mile buffers of MOUDs). We assume that access to MOUDs is critical and requires high regularity, and that walking is the most likely option during COVID. This guides the parameter specification of MOUD Service Areas (and is also backed up by some literature in this space, though much more is needed.) 5.1 Load Spatial Data Let’s first reload our spatial data – this will be the MOUD points, plus the master zip code spatial file. #library(sf) #library(tmap) points &lt;- st_read(&quot;data/methadoneMOUD.geojson&quot;) ## Reading layer `methadoneMOUD&#39; from data source ## `/Users/maryniakolak/Code/Intro2RSpatialMed/data/methadoneMOUD.geojson&#39; using driver `GeoJSON&#39; ## Simple feature collection with 25 features and 8 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -87.7349 ymin: 41.68698 xmax: -87.57673 ymax: 41.9533 ## Geodetic CRS: WGS 84 areas &lt;- st_read(&quot;data/geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5.shp&quot;) ## Reading layer `geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5&#39; from data source ## `/Users/maryniakolak/Code/Intro2RSpatialMed/data/geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 61 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.94011 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304 ## Geodetic CRS: WGS84(DD) dim(points) ## [1] 25 9 dim(areas) ## [1] 61 5 head(points) ## Simple feature collection with 6 features and 8 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -87.72186 ymin: 41.88474 xmax: -87.63409 ymax: 41.9533 ## Geodetic CRS: WGS 84 ## X Name Address City State Zip ## 1 3 Soft Landing Interventions/DBA Symetria Recovery of Lakeview 3934 N. Lincoln Ave. Chicago IL 60613 ## 2 4 PDSSC - Chicago, Inc. 2260 N. Elston Ave. Chicago IL 60614 ## 3 5 Center for Addictive Problems, Inc. 609 N. Wells St. Chicago IL 60654 ## 4 6 Family Guidance Centers, Inc. 310 W. Chicago Ave. Chicago IL 60654 ## 5 7 A Rincon Family Services 3809 W. Grand Ave. Chicago IL 60651 ## 6 8 * 140 N. Ashland Ave. Chicago IL 60607 ## fullAdd geo_method geometry ## 1 3934 N. Lincoln Ave. Chicago IL 60613 census POINT (-87.67818 41.9533) ## 2 2260 N. Elston Ave. Chicago IL 60614 census POINT (-87.67407 41.92269) ## 3 609 N. Wells St. Chicago IL 60654 census POINT (-87.63409 41.89268) ## 4 310 W. Chicago Ave. Chicago IL 60654 census POINT (-87.63636 41.89657) ## 5 3809 W. Grand Ave. Chicago IL 60651 census POINT (-87.72186 41.90435) ## 6 140 N. Ashland Ave. Chicago IL 60607 osm POINT (-87.66725 41.88474) head(areas) ## Simple feature collection with 6 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -87.80649 ymin: 41.88747 xmax: -87.59852 ymax: 41.93228 ## Geodetic CRS: WGS84(DD) ## objectid shape_area shape_len zip geometry ## 1 33 106052287 42720.04 60647 MULTIPOLYGON (((-87.67762 4... ## 2 34 127476051 48103.78 60639 MULTIPOLYGON (((-87.72683 4... ## 3 35 45069038 27288.61 60707 MULTIPOLYGON (((-87.785 41.... ## 4 36 70853834 42527.99 60622 MULTIPOLYGON (((-87.66707 4... ## 5 37 99039621 47970.14 60651 MULTIPOLYGON (((-87.70656 4... ## 6 38 23506056 34689.35 60611 MULTIPOLYGON (((-87.61401 4... 5.2 Transform Projections First we need to switch to a projection that uses distance in feet or meters as a metric. We’ll use EPSG:3435 from the first tutorial. To find which EPSG was recommended, I searched “EPSG Illinois feet” and EPSG:3435 came up as a viable candidate. So, we use that for our new, projected CRS. areas &lt;- st_transform(areas, 3435) points &lt;- st_transform(points, 3435) We may want to once again confirm they are plotting correctly: tm_shape(areas) + tm_polygons() + tm_shape(points) + tm_dots(size = 1) 5.3 Count resources by area One way of understanding resource inequity is by thinking about how many resources exist in a neighborhood. First, give the points the attributes of the polygons they are in. Inspect. pipr &lt;- st_join(points, areas) head(pipr) ## Simple feature collection with 6 features and 12 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: 1150707 ymin: 1901294 xmax: 1174632 ymax: 1926255 ## Projected CRS: NAD83 / Illinois East (ftUS) ## X Name Address City State Zip ## 1 3 Soft Landing Interventions/DBA Symetria Recovery of Lakeview 3934 N. Lincoln Ave. Chicago IL 60613 ## 2 4 PDSSC - Chicago, Inc. 2260 N. Elston Ave. Chicago IL 60614 ## 3 5 Center for Addictive Problems, Inc. 609 N. Wells St. Chicago IL 60654 ## 4 6 Family Guidance Centers, Inc. 310 W. Chicago Ave. Chicago IL 60654 ## 5 7 A Rincon Family Services 3809 W. Grand Ave. Chicago IL 60651 ## 6 8 * 140 N. Ashland Ave. Chicago IL 60607 ## fullAdd geo_method objectid shape_area shape_len zip ## 1 3934 N. Lincoln Ave. Chicago IL 60613 census 53 53990895 31196.32 60613 ## 2 2260 N. Elston Ave. Chicago IL 60614 census 32 94460632 50587.35 60614 ## 3 609 N. Wells St. Chicago IL 60654 census 55 15869961 17119.70 60654 ## 4 310 W. Chicago Ave. Chicago IL 60654 census 54 31598157 24208.70 60610 ## 5 3809 W. Grand Ave. Chicago IL 60651 census 37 99039621 47970.14 60651 ## 6 140 N. Ashland Ave. Chicago IL 60607 osm 16 106718949 42663.20 60612 ## geometry ## 1 POINT (1162460 1926255) ## 2 POINT (1163663 1915110) ## 3 POINT (1174632 1904257) ## 4 POINT (1174003 1905671) ## 5 POINT (1150707 1908328) ## 6 POINT (1165627 1901294) You should have the same number of rows in pipr as you do in points. If not, there is something off. You may need to go back to troubleshoot. In an earlier version of this lab, I used a saved, written geojson file of the zip codes, and it would not render properly. Here, we load in the original shapefile at the beginning of the tutorial to avoid that error. dim(pipr) ## [1] 25 13 dim(points) ## [1] 25 9 dim(areas) ## [1] 61 5 5.3.1 Count # per Area Next, count the number per area. The frequency should be logical according to the map you made earlier. Sometimes, I’ve found bugs where the numbers are multipled by some factor; this can be checked by looking at dimension disparities, as noted above. ptcount &lt;- as.data.frame(table(pipr$Zip)) head(ptcount) ## Var1 Freq ## 1 60607 2 ## 2 60608 1 ## 3 60609 1 ## 4 60613 1 ## 5 60614 1 ## 6 60615 1 How could improve on this step if you used dplyr? Aggregation Tip: What if you have an attribute value you’d like to aggregate? For example, average units of affordable housing facility by zip? Try aggregate(pip$attribute, by = list(pip$geoid), mean) but build on with a tidy sensibility… Now we can rename our attributes: names(ptcount) &lt;- c(&quot;zip&quot;, &quot;MetClnc&quot;) head(ptcount) ## zip MetClnc ## 1 60607 2 ## 2 60608 1 ## 3 60609 1 ## 4 60613 1 ## 5 60614 1 ## 6 60615 1 And finally, merge back to your master zip file: head(areas) ## Simple feature collection with 6 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 1127607 ymin: 1902374 xmax: 1184320 ymax: 1918596 ## Projected CRS: NAD83 / Illinois East (ftUS) ## objectid shape_area shape_len zip geometry ## 1 33 106052287 42720.04 60647 MULTIPOLYGON (((1162711 191... ## 2 34 127476051 48103.78 60639 MULTIPOLYGON (((1149304 191... ## 3 35 45069038 27288.61 60707 MULTIPOLYGON (((1133505 190... ## 4 36 70853834 42527.99 60622 MULTIPOLYGON (((1165664 190... ## 5 37 99039621 47970.14 60651 MULTIPOLYGON (((1154895 190... ## 6 38 23506056 34689.35 60611 MULTIPOLYGON (((1180097 190... areas&lt;- merge(areas, ptcount, by=&quot;zip&quot;, all = TRUE) head(areas) ## Simple feature collection with 6 features and 5 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 1173038 ymin: 1889918 xmax: 1183259 ymax: 1902959 ## Projected CRS: NAD83 / Illinois East (ftUS) ## zip objectid shape_area shape_len MetClnc geometry ## 1 60601 27 9166246 19804.58 NA MULTIPOLYGON (((1177742 190... ## 2 60602 26 4847125 14448.17 NA MULTIPOLYGON (((1181226 190... ## 3 60603 19 4560229 13672.68 NA MULTIPOLYGON (((1179499 190... ## 4 60604 48 4294902 12245.81 NA MULTIPOLYGON (((1174763 189... ## 5 60605 20 36301276 37973.35 NA MULTIPOLYGON (((1178341 189... ## 6 60606 31 6766411 12040.44 NA MULTIPOLYGON (((1174681 190... Quickly map to inspect: tm_shape(areas) + tm_polygons(col = &quot;gray80&quot;) + tm_shape(areas) + tm_polygons(col = &quot;MetClnc&quot;, style = &quot;pretty&quot;, alpha = 0.8) + tm_shape(points) + tm_dots(size = 0.5) 5.4 Buffer Data Next, lets create a walkable buffer of one mile, or 5280 feet, for our MOUD provider locations. Individuals residing in places outside of that walkable area may have difficulty accessing this medication during crises, like a pandemic. # Create 1mile buffers for each point ptbuffers &lt;- st_buffer(points, 5280) head(ptbuffers) ## Simple feature collection with 6 features and 8 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 1145427 ymin: 1896014 xmax: 1179912 ymax: 1931535 ## Projected CRS: NAD83 / Illinois East (ftUS) ## X Name Address City State Zip ## 1 3 Soft Landing Interventions/DBA Symetria Recovery of Lakeview 3934 N. Lincoln Ave. Chicago IL 60613 ## 2 4 PDSSC - Chicago, Inc. 2260 N. Elston Ave. Chicago IL 60614 ## 3 5 Center for Addictive Problems, Inc. 609 N. Wells St. Chicago IL 60654 ## 4 6 Family Guidance Centers, Inc. 310 W. Chicago Ave. Chicago IL 60654 ## 5 7 A Rincon Family Services 3809 W. Grand Ave. Chicago IL 60651 ## 6 8 * 140 N. Ashland Ave. Chicago IL 60607 ## fullAdd geo_method geometry ## 1 3934 N. Lincoln Ave. Chicago IL 60613 census POLYGON ((1167740 1926255, ... ## 2 2260 N. Elston Ave. Chicago IL 60614 census POLYGON ((1168943 1915110, ... ## 3 609 N. Wells St. Chicago IL 60654 census POLYGON ((1179912 1904257, ... ## 4 310 W. Chicago Ave. Chicago IL 60654 census POLYGON ((1179283 1905671, ... ## 5 3809 W. Grand Ave. Chicago IL 60651 census POLYGON ((1155987 1908328, ... ## 6 140 N. Ashland Ave. Chicago IL 60607 osm POLYGON ((1170907 1901294, ... Inspect immediately: tm_shape(areas) + tm_borders() + tm_shape(ptbuffers) + tm_borders(col = &quot;blue&quot;) 5.5 Count buffers by area We know that MOUD locations are accessible up to one mile away. So, a total count of resources by area may be too restrictive. Let’s calculate how many walkable MOUD clinics are in each zip code. Or, how many buffers are in each area… bufferct &lt;- lengths(st_intersects(areas, ptbuffers)) head(bufferct) ## [1] 2 2 1 1 1 2 Stick buffer totals back to the zip master file: # Stick buffer totals back to the census master file areas &lt;- cbind(areas,bufferct) head(areas) ## Simple feature collection with 6 features and 6 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 1173038 ymin: 1889918 xmax: 1183259 ymax: 1902959 ## Projected CRS: NAD83 / Illinois East (ftUS) ## zip objectid shape_area shape_len MetClnc bufferct geometry ## 1 60601 27 9166246 19804.58 NA 2 MULTIPOLYGON (((1177742 190... ## 2 60602 26 4847125 14448.17 NA 2 MULTIPOLYGON (((1181226 190... ## 3 60603 19 4560229 13672.68 NA 1 MULTIPOLYGON (((1179499 190... ## 4 60604 48 4294902 12245.81 NA 1 MULTIPOLYGON (((1174763 189... ## 5 60605 20 36301276 37973.35 NA 1 MULTIPOLYGON (((1178341 189... ## 6 60606 31 6766411 12040.44 NA 2 MULTIPOLYGON (((1174681 190... Map density of buffers per census area: tm_shape(areas) + tm_polygons(col = &quot;bufferct&quot;, palette = &quot;BuGn&quot;, n=5, style = &quot;jenks&quot;) + tm_shape(ptbuffers) + tm_fill(col = &quot;gray90&quot;, alpha=0.6) + tm_shape(points) + tm_dots(col = &quot;gray10&quot;, size = 0.3) 5.6 Integrate &amp; Explore Let’s review: our master area file now has total number resources by zip and total number of walkable service areas by zip. Using your new spatial file, see if you can answer some of these quetions using various queries: Which zip codes have high rates of COVID and are not within a walking distance of a methadone MOUD? Which zip codes have worse access to affordable rental units, low educational rates, and less walkable access to MOUDs? What is the demographic and racial/ethnic characteristics of areas most vulnerable to high COVID rates in September 2020? Generate different maps and outputs to drive your thinking and defend your hypothesis formation. "]]
