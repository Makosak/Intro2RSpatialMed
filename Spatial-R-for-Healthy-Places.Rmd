--- 
title: "Intro to R-Spatial for Healthy Places"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "This was developed for a workshop at the 2021 R-Medicine Conference by the Health Regions & Policies Lab at the University of Chicago ."
---

## Background {-}

This suite of tutorials was developed for a workshop at the 2021 R-Medicine Conference by the [Healthy Regions & Policies Lab](http://voices.uchicago.edu/herop/) at the University of Chicago. This workbook is a quick (3-hour) overview of mapping, GIScience, and spatial analysis basics for health professionals. The workbook was compiled by Marynia Kolak, and the overview for each section is led by Susan Paykin in the live version.  

Some coding snippets & data examples are from the phenomenal team of the [Opioid Environment Toolkit](https://geodacenter.github.io/opioid-environment-toolkit) (Moksha Menghaney, Qinyun Lin, Angela Li). The overall approach follows the Center for Spatial Data Science paradigm, led by Luc Anselin & Julia Koschinsky.

### Environment Setup {-}

A basic understanding of R is assumed. This workshop requires several packages, which can be installed from CRAN:

```{r eval=FALSE}
install.packages("sf", "tmap", "tidygeocoder")
```

For Mac users, check out https://github.com/r-spatial/sf for additional tips if you run into errors when installing the **sf** package. Using homebrew to install **gdal** usually fixes any remaining issues.



<!--chapter:end:index.Rmd-->

# Intro to Spatial Data {#intro}

In the workshop, we learned about:

- What is Spatial Data?
- What is the `sf` framework for R?

To delve in further, let's see some spatial data in action.

We'll work with the **sf** library first.

```{r load-libraries, warning=FALSE}
library(sf)
```

## Load Spatial Data

First load in the shapefile. Remember, this type of data is actually comprised of multiple files. All need to be present in order to read correctly.
```{r load-spatial-data}
Chi_tracts = st_read("data/geo_export_aae47441-adab-4aca-8cb0-2e0c0114096e.shp")
```

## Non-Spatial & Spatial Views

Always inspect data when loading in. First we look at a non-spatial view.

```{r non-spatial-view}
head(Chi_tracts)
```

Note the last column -- this is a spatially enabled column. The data is no longer a 'shapefile' but an `sf' object, comprised of polygons.

We can use a baseR function to view the spatial dimension. The `sf` framework enables previews of each attribute in our spatial file. 

```{r spatial-view}
plot(Chi_tracts)
```

## Spatial Data Structure

Check out the data structure of this file... What object is it?

```{r data-structure}
str(Chi_tracts)

```

Check out the coordinate reference system. What is it? What are the units?

```{r }
st_crs(Chi_tracts)
```

## Exploring Coordinate Reference Systems

Lets see how switching CRS changes our object. First we'll try the Mollweide coordinate reference system that does a good job preserving area across the globe. 

To transform our CRS, we use the `st_transform` function. To plot, we use baseR again but with some paremeter updates. Finally, we check out the CRS of our new object. What are the units? Any other details to note? Will this be appropriate for our spatial analysis?

```{r }
Chi_tracts.moll <- st_transform(Chi_tracts, crs = "+proj=moll")
plot(st_geometry(Chi_tracts.moll), border = "gray", lwd = 2, main = "Mollweide", sub="preserves areas")
st_crs(Chi_tracts.moll)
```

Next, we'll try the Winkel CRS, which is a compromise projection that facilitates minimal distortion for area, distance, and angles. We use the same approach, recyling the code with new inputs.

```{r }
Chi_tracts.54019 = st_transform(Chi_tracts, 54019)
plot(st_geometry(Chi_tracts.54019), border = "gray", lwd = 2, main = "Winkel", sub="minimal distortion")
st_crs(Chi_tracts.54019)

```


We could also try a totally different projection, to see how that changes our spatial object. Let's use the "Old Hawaiian UTM Zone 4n" projection, with the EPSG identified from an online search. How does this fare?

```{r }
Chi_tracts.Hawaii = st_transform(Chi_tracts, 102114)
plot(st_geometry(Chi_tracts.Hawaii), border = "gray", lwd = 2, main = "Old Hawaiian UTM Zone 4N", sub="wrong projection!")
```

Finally.. let's choose a projection that is focused on Illinois, and uses distance as feet or meters, to make it a bit more accessible for our work. EPSG:3435 is a good fit:

```{r }
Chi_tracts.3435 <- st_transform(Chi_tracts, 3435)
st_crs(Chi_tracts.3435)

plot(st_geometry(Chi_tracts.3435), border = "gray", lwd = 2, main = "NAD83 / Illinois East (ftUS)", sub="topo mapping & survey use")
```

## Refine Basic Map

Now we'll switch to a more extensive cartographic mapping package, `tmap`. We approach mapping with one layer at a time. Always start with the object you want to map by calling it with the `tm_shape` function. Then, at least one descriptive/styling function follows. There are hundreds of variations and paramater specifications, so take your time in exploring `tmap` and the options.

Here we style the tracts with some semi-transparent borders.
```{r }
library(tmap)

tm_shape(Chi_tracts) + tm_borders(alpha=0.5) 
```

Next we fill the tracts with a light gray, and adjust the color and transparency of borders. We also add a scale bar, positioning it to the left and having a thickness of 0.8 units, and turn off the frame.

```{r }
tm_shape(Chi_tracts) + tm_fill(col = "gray90") + tm_borders(alpha=0.2, col = "gray10") +
  tm_scale_bar(position = ("left"), lwd = 0.8) +
  tm_layout(frame = F)
```

Check out https://rdrr.io/cran/tmap/man/tm_polygons.html for more ideas!

## Arrange multiple maps

Sometimes we want to look at multiple maps at once. Write your mapping function to a new variable, and then call that variable in order of desire using the `tmap_arrange` function. Hint: this is just one of many! ways to map multiples using `tmap`... see if you can uncover more in the documentation.

```{r }
tracts.4326 <- tm_shape(Chi_tracts) + tm_fill(col = "gray90") +
  tm_layout(frame = F, title = "EPSG 4326")

tracts.54019 <- tm_shape(Chi_tracts.54019) + tm_fill(col = "gray90") +  tm_layout(frame = F, title = "EPSG 54019")

tmap_arrange(tracts.4326, tracts.54019)
```

## Interactive Mode

So far, we've been plotting static maps. We can also switch to an interactive map that uses a Leaflet widget by switching the `tmap_mode()` parameter specification from "plot" to "view." It's on "plot" as default.

```{r warning=FALSE}
tmap_mode("view")
```

Map the same map as before, and check out the interaction! 

```{r warning=FALSE}
tm_shape(Chi_tracts) + tm_fill(col = "gray90") + tm_borders(alpha=0.2, col = "gray10") +
  tm_scale_bar(position = ("left"), lwd = 0.8) +
  tm_layout(frame = F)
```


The tracts are not transparent enough, so we update that here. You can also click the box on the left side to try out other basemaps. See if you can find out how to add a basemap to a static/plotted map, using `tmap` documentation...

```{r warning=FALSE}
tm_shape(Chi_tracts) + tm_fill(col = "gray90", alpha = 0.5) + tm_borders(alpha=0.2, col = "gray10") + tm_scale_bar(position = ("left"), lwd = 0.8) +  tm_layout(frame = F)
```

We revert back to `plot` mode for now.

```{r }
tmap_mode("plot")
```

## Overlay Zip Code Boundaries

How do census tract areas correspond to zip codes? While tracts better represent neighborhoods, often times we are stuck with zip code level scale in healh research. Here we'll make a reference map to highlight tract distribution across each zip code.

First, we read in zip code boundaries. This data was downloaded directly from the *City of Chicago Data Portal* as a shapefile.

```{r }
Chi_Zips = st_read("data/geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5.shp")

```

Next, we layer the new shape in -- on top of the tracts. We use a thicker border, and try out a new color. Experiment!

```{r }

## FIRST LAYER: CENSUS TRACT BOUNADRIES
tm_shape(Chi_tracts.3435) + tm_fill(col = "gray90") +
  tm_borders(alpha=0.2, col = "gray10") + 

## SECOND LAYER: ZIP CODE BOUNDARIES WITH LABEL
tm_shape(Chi_Zips) + tm_borders(lwd = 2, col = "#0099CC") +
  tm_text("zip", size = 0.7) +
  
## MORE CARTOGRAPHIC STYLE
  tm_scale_bar(position = ("left"), lwd = 0.8) +
  tm_layout(frame = F)
```

## More Resources {-}

On spatial data basics & sf:

- https://geocompr.robinlovelace.net/intro.html

- https://geodacenter.github.io/opioid-environment-toolkit/spatial-data-introduction.html 

On projections:

- https://desktop.arcgis.com/en/arcmap/10.3/guide-books/map-projections/projection-basics-for-gis-professionals.htm

- https://geocompr.robinlovelace.net/reproj-geo-data.html 

- https://datacarpentry.org/organization-geospatial/03-crs/index.html

On tmap:

- https://cran.r-project.org/web/packages/tmap/vignettes/tmap-getstarted.html 

- https://geocompr.robinlovelace.net/adv-map.html 

<!--chapter:end:01-intro.Rmd-->

# Map Neighborhoods

When considering the health of persons, we have to also consider the neighborhood environment. Sometimes this is looking at neighborhood level health outcomes, like premature mortality at the census tract scale, or cumulative COVID rates by zip code. Sometimes we're interested in neighborhood factors like  poverty, access to affordable housing, or distance to nearest health provider, or pollution-emitting facility. These measurements of the "social determinants of health" at the neighborhood scale are increasingly urgent in modern public health thinking, and are thought to drive and/or reinforce racial, social, and spatial inequity

In this module, we'll learn about the basics of thematic mapping -- known as *choropleth mapping* -- to visualize neighborhood level health phenomena. This will allow you to begin the process of exploratory spatial data analysis and hypothesis generation & refinement.

## Clean Attribute Data

Let's consider COVID-19 cases by zip code in Chicago. We'll upload and inspect a summary of cases from the *Chicago Data Portal* first:

```{r}
COVID <- read.csv("data/COVID-19_Cases__Tests__and_Deaths_by_ZIP_Code.csv")

head(COVID)
```

Each row corresponds to a zip code at a different week. This data thus exists as a "long" format, which doesn't work for spatial analysis. We need to convert to "wide" format, or at the very least, ensure that each zip code corresponds to one row. 

To simplify, let's identify the last week of the dataset, and then subset the data frame to only show that week. We will be interested in the cumulative case rate. Following is one way of doing this -- can you think of another way? Try out different approaches of reshaping data to test your R and "tidy" skills.

```{r}
## How many weeks are in our dataset?
range(as.numeric(COVID$Week.End))

## Subset & inspect to week 39.
COVID.39 <- subset(COVID, COVID$Week.Number == "39")
head(COVID.39)
```

To clean our data a bit, we'll just keep the zip code name, and cumulative case rate for the week of September 20th, 2020.

```{r}
COVID.39f <- COVID.39[,c("ZIP.Code", "Case.Rate...Cumulative")]

head(COVID.39f)
```

## Merge Spatial Data

Next, let's merge this data to our zip code master spatial file. Reload if necessary:

```{r}
library(sf)

Chi_Zips = st_read("data/geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5.shp")

head(Chi_Zips)
```

Next, merge on zip code ID. The key in the Chi_Zips object is `zip`, whereas the key for the COVID data is `ZIP.code`. Always merge non-spatial *to* spatial data, not the other way around. Think of the spatial file as your master file that you will continue to add on to...

```{r}
Chi_Zipsf <- merge(Chi_Zips, COVID.39f, by.x = "zip", by.y = "ZIP.Code")

head(Chi_Zipsf)
```

## Quantile Maps

Starting with a "classic epi" approach, let's look at case rates as quantiles. We use the `tmap` library, and update the choropleth data classification using the `style` parameter. We use the Blue-Purple palette, or `BuPu`, from Colorbrewer. 


**Colorbrewer Tip:** To display all Colorbrewer palette options, load the `RColorBrewer` library and run `display.brewer.all()` -- or just Google "R Colorbrewer palettes."

```{r message=FALSE, warning=FALSE}
library(tmap)

tm_shape(Chi_Zipsf ) +
  tm_polygons("Case.Rate...Cumulative", 
              style="quantile", pal="BuPu",
              title = "COVID Case Rate") 
```


Let's try tertiles:

```{r message=FALSE}
tm_shape(Chi_Zipsf ) +
  tm_polygons("Case.Rate...Cumulative", 
              style="quantile", n=3, pal="BuPu",
              title = "COVID Case Rate") 
```

## Standard Deviation Maps

While quantiles are a nice start, let's classify using a standard deviation map. Standard deviation is a statistical technique type of map based on how much the data differs from the mean. 

```{r message=FALSE}
tm_shape(Chi_Zipsf ) +
  tm_polygons("Case.Rate...Cumulative", 
              style="sd", pal="BuPu",
              title = "COVID Case Rate") 
```

## Jenks Maps

Another approach of data classification is natural breaks, or jenks. This approach looks for "natural breaks" in the data using a univariate clustering algorithm.

```{r message=FALSE}
tm_shape(Chi_Zipsf ) +
  tm_polygons("Case.Rate...Cumulative", 
              style="jenks", pal="BuPu",
              title = "COVID Case Rate") 
```

The first bin doesn't seem very intuitive. Let's try 4 bins instead of 5 by changing the `n` parameter. In this version, we'll also had a histogram and scale bar, and move the legend outside the frame to make it easier to view.

```{r message=FALSE}
tm_shape(Chi_Zipsf ) +
  tm_polygons("Case.Rate...Cumulative", 
              style="jenks", pal="BuPu",
              legend.hist=T, n=4,
              title = "COVID Case Rate", ) + 
  tm_scale_bar(position = "left") + 
  tm_layout(legend.outside = TRUE, legend.outside.position = "right")

```

## Integrate More Data

To explore potential disparities in COVID health outcomes, let's bring in pre-cleaned demographic, racial, and ethnic data from the [Opioid Environment Policy Scan database](https://geodacenter.github.io/opioid-policy-scan/). This data is orginally sourced from the American Community Survey 2018 5-year estimate, which you could also pull using the `tidycensus`.

```{r}
CensusVar <- read.csv("data/DS01_Z.csv")
head(CensusVar)
```

Merge to our master Zip Code dataset.

```{r}
Chi_Zipsf <- merge(Chi_Zipsf, CensusVar, by.x = "zip", by.y = "ZCTA")

head(Chi_Zipsf)
```

## Thematic Map Panel
To facilitate data discovery, we likely want to explore multiple maps at once. Here we'll generate maps for multiple variables, and plot them as a map panel.

Can you think of more efficient ways to run this code? There are also other `tmap` tricks to optimize this further, so enjoy your journey!

```{r message=FALSE, warning=FALSE}
COVID <- tm_shape(Chi_Zipsf) + tm_fill("Case.Rate...Cumulative", 
              style="jenks", pal="BuPu", n=4, title = "COVID Rt") + 
  tm_layout(frame = F)

Senior <- tm_shape(Chi_Zipsf) + tm_fill("ovr65P", 
              style="jenks", pal="BuPu", n=4) +
  tm_layout(frame = F)

NoHS <- tm_shape(Chi_Zipsf) + tm_fill("noHSP", 
              style="jenks", pal="BuPu", n=4) +
  tm_layout(frame = F)

BlkP <- tm_shape(Chi_Zipsf) + tm_fill("blackP", 
              style="jenks", pal="BuPu", n=4) +
  tm_layout(frame = F)

Latnx <- tm_shape(Chi_Zipsf) + tm_fill("hispP", 
              style="jenks", pal="BuPu", n=4) +
  tm_layout(frame = F)

WhiP <- tm_shape(Chi_Zipsf) + tm_fill("whiteP", 
              style="jenks", pal="BuPu", n=4) +
  tm_layout(frame = F)

tmap_arrange(COVID, Senior, NoHS, BlkP, Latnx, WhiP)
```

From the results, we see that cumulative COVID outcomes for one week in September 2020 seemed to have some geographic correlation with the Latinx/Hispanic community in Chicago. At the same time, low high school diploma rates are also concentrated in these areas, and there is some intersection with other variables considered. What are additional variables you could bring in to refine your approach? Perhaps percentage of essential workers; a different age group; internet access? What about linking in health outcomes like Asthma, Hypertension, and more at a similar scale?

In modern spatial epidemiology, associations must never be taken at face value. For example, we know that it is not "race" but "racism" that drives multiple health disparities -- simply looking at a specific racial/ethnic group is not enough. Thus exploring multiple variables and nurturing a curiosity to understand these complex intersections will support knowledge discovery.

## Write Data
We're  done! Well... not so fast. Let's save the data so we don't have to run the codebook again to access the data. Here, we'll save as a `geojson` file. This spatial format is more forgiving with long column names, which is a long-standing challenge with shapefiles. 

```{r eval=FALSE}
#st_write(Chi_Zipsf, "data/ChiZipMaster.geojson", driver = "GeoJSON", append= FALSE)
```

## More Resources {-}

For choropleth mapping in R:

- https://spatialanalysis.github.io/lab_tutorials/4_R_Mapping.html

<!--chapter:end:02-choropleth.Rmd-->

# Adding Resources

In addition to areal data, we can also extract information from individual locations. Locations, when measured as points, can include things like:

- **Health providers:** Hospitals, Clinics, Pharmacies, Mental health providers, Medication for opioid use disorder providers
- **Area resources:** Grocery stores & Supermarkets, Playgrounds, Daycare centers, Schools, Community centers
- **Area challenges:** Crime, Superfund sites, Pollution-emitting facilities

Points can also represent *people*, like individual patients residing in an area. Because individual locations for persons is protected health information, we'll focus on point data as resources in the chapter. However, you can reuse the code snippets in this workshop to wrangle patient-level data the same way in a secure environment, under the guidance of your friendly IRB ethics board. 

In this example, we'll extend our Chicago example. We'll identify areas with high COVID rates, low geographic access to methadone maintenance therapy, and less access to affordable rental housing units managed by the city. We are interested in locating zip codes that may be especially vulnerable to persons with opioid use disorder who use MOUDs. (This is oversimplified, but our example to work with.)

## Geocode
If you start with only addresses, you'll need to geocode. Our methadone maintenance provider dataset is only available as such. Addresses are comprised of characeters that reference a specific place. We will use the network topology service of a *Geocoder* to translate that address to a coordinate in some CRS. 

First we load the `tidygeocoder` to get our geocoding done. Note, this uses the interent to process, so is not suitable for HIPPA protected data like individual, living person addresses. For offline geocoders, check out `Pelias` or `ESRI`.

```{r warning=FALSE}
library(tidygeocoder)
```

Let's read in and inspect data for methadone maintenance providers. Note, these addresses were made available by SAMSHA, and are known as publicly available information. An additional analysis could call each service to check on access to medication during COVID in Septmber 2020, and the list would be updated further.

```{r}
methadoneClinics <- read.csv("data/chicago_methadone_nogeometry.csv")
head(methadoneClinics)
```

Let's geocode one address first, just to make sure our system is working. We'll use the "cascade" method which use the US Census and OpenStreetMap geocoders. These two services are the main options with `tidygeocoder`.

```{r}
sample <- geo("2260 N. Elston Ave. Chicago, IL", lat = latitude, long = longitude, method = 'cascade')
head(sample)
```

As we prepare for geocoding, check out the structure of the dataset. Do we need to change anything? The data should be a character to be read properly. 

```{r}
str(methadoneClinics)
```

We need to clean the data a bit. We'll add a new column for a full address, as required by the geocoding service. When you use a geocoding service, be sure to read the documentation and understand how the data needs to be formatted for input.

```{r}
methadoneClinics$fullAdd <- paste(as.character(methadoneClinics$Address), 
                                  as.character(methadoneClinics$City),
                                  as.character(methadoneClinics$State), 
                                  as.character(methadoneClinics$Zip))
```


We're ready to go! Batch geocode with one function, and inspect:

```{r}
geoCodedClinics <-  geocode(methadoneClinics,
address = 'fullAdd', lat = latitude, long = longitude, method = 'cascade')

head(geoCodedClinics)
```

There were two that didn't geocode correctly. You can inspect further. This could involve a quick check for spelling issues; or, searching the address and pulling the lat/long using Google Maps and inputting manually. Or, if we are concerned it's a human or unknown error, we could omit. For this exercise we'll just omit the two clinics that didn't geocode correctly.

```{r}
geoCodedClinics2 <- na.omit(geoCodedClinics)
```

## Convert to Spatial Data

This is not spatial data yet! To convert a static file to spatial data, we use the powerful `st_as_sf` function from `sf`. Indicate the x,y parameters (=longitude, latitude) and the coordinate reference system used. Our geocoding service used the standard **EPSG:4326**, so we input that here.

```{r warning = FALSE}
library(sf)

methadoneSf <- st_as_sf(geoCodedClinics2, 
                        coords = c("longitude", "latitude"),
                        crs = 4326)
```

## Basic Map of Points 

For a really simple map of points -- to ensure they were geocoded and converted to spatial data correctly, we use `tmap`. We'll use the interactive version to view.

```{r warning = FALSE, message=FALSE}
library(tmap)

tmap_mode("view")

tm_shape(methadoneSf) + tm_dots() 
```

If your points didn't plot correctly:

- Did you flip the longitude/latitude values?
- Did you input the correct CRS?

Those two issues are the most common errors.

## Overlay Points & Style

Let's add our zip code map from the previous module. First load the data, then overlay.

```{r}
Chi_Zipsf <- st_read("data/ChiZipMaster1.geojson")
```

With this overlay, we'll add a "hack" to include the methadone clinic points in a legend.

```{r}
tmap_mode("plot")

## 1st layer (gets plotted first)
tm_shape(Chi_Zipsf) + tm_fill("Case.Rate...Cumulative", 
              style="jenks", pal="BuPu", n=4, title = "COVID Rt") + 
  
  ## 2nd layer (overlay)
  tm_shape(methadoneSf) + tm_dots(size = 0.2, col = "gray20") +
  
  ## "Hack" a manual symbology for dots in the legend
  tm_add_legend("symbol", col = "gray20", size = .2, labels = "Methadone MOUD") +
  
  ## Cartographic Styling
  tm_layout(legend.outside = TRUE, legend.outside.position = "right")
```

## Integrate More Data

From here, we can integrate more data. Let's try a different point dataset -- Affordable Rental Housing Developments, as made available by the *City of Chicago Data Portal*. This could be interesting for a number of different reasons -- maybe we hypothesize better outcomes are associated with better access to affordable housing options? Or, we hypothesize the opposite, that mean distance to more population dense housing locations is vulnerable to airborne disease? 

For this example, we'll think about this dataset as access to secure and affordable housing. Persons with lower incomes residing in places with fewer developments may be more vulnerable to housing insecurity -> impacts health.

```{r, warning=FALSE}
AffHousing <- read.csv("data/Affordable_Rental_Housing_Developments.csv")

head(AffHousing)
```

There were a few data points with odd inputs and null values. Remember, we can't convert any null values to spatial coordinates. Again, in an ideal context, you would explore and understand what is happening, systematically. In our experiment, we'll omit nulls.

```{r, warning=FALSE}
AffHousing <- na.omit(AffHousing)
```

Look at the structure of the object. 

```{r, warning=FALSE}
str(AffHousing)
```

In this dataset, we can see coordinate information is already included -- twice! You're looking at 2 different types of coordinate systems. We'll use "Longitude" and "Latitude" to represent X,Y and an ESPG of 4326. We're guessing, and hopeful.

```{r, warning=FALSE}
AffHousingSf <- st_as_sf(AffHousing, 
                        coords = c("Longitude", "Latitude"),
                        crs = 4326)
```

We can now map the data for a quick view -- does this look like Chicago, hopefully?

```{r, warning=FALSE}
tm_shape(AffHousingSf) + tm_dots() 
```

## Graduated Symbology

Previously we mapped points as dots. We literally used the `tm_dots()` function to do so. Another option is changing the size of the point, according to some attribute of the data. In this dataset, we see an attribute field that gives us the total number of units per housing site. Let's use a graduated symbology, with the `tm_bubbles()` function, to map these points. That way points with more units will be bigger, and not all places are weighted the same visually.

```{r}
tm_shape(Chi_Zipsf) + tm_polygons(col = "gray80") +
  tm_shape(AffHousingSf) + tm_bubbles("Units", col = "purple") 
```

## Style Final Map

Let's pull what we learned in the last tutorial, and map everything at once. Which zip codes are the most vulnerable to persons with OUD during the pandemic in September 2020, based on the information we have here?

```{r}
## Zip Codes with Labels
tm_shape(Chi_Zipsf) + tm_fill("Case.Rate...Cumulative", 
      style="jenks", pal="BuPu", n=4, title = "COVID Rt") +
      tm_text("zip", size = 0.7) +

  ## Affordable Housing Units
  tm_shape(AffHousingSf) + tm_bubbles("Units") + 
  
  ## Methadone MOUD
  tm_shape(methadoneSf) + tm_dots(size = 0.2, col = "gray20") +
  
  ## Cartographic Styling
  tm_add_legend("symbol", col = "gray20", size = .2, labels = "Methadone MOUD") + tm_layout(legend.outside = TRUE, legend.outside.position = "right")
```

In RStudio, you could zoom into the plot you created to get a better view. Save as an image, or save as a webpage!

Save any data you need from this session.

```{r}
#st_write(methadoneSf, "data/methadoneMOUD1.shp")
```

<!--chapter:end:03-overlaypoints.Rmd-->

# Calculate Spatial Metrics

While we've generated some nice visualizations, we need insights quantified as metrics at the neighborhood level. Don't start this step until you have a good idea of what you need. Visualizing and exploring the data in depth is best practice.

For our purposes, we're interested in developing spatial access metrics with a container method approach. At the end of this tutorial, we'll generate the following new variables:

- Total number of Methadone Maintenance MOUD by zip code
- Total number of Walkble MOUD Service Areas by zip code

Plus, we will have a new spatial layer, that includes the actual service areas (ie. 1-mile buffers of MOUDs). We assume that access to MOUDs is critical and requires high regularity, and that walking is the most likely option during COVID. This guides the parameter specification of MOUD Service Areas (and is also backed up by some literature in this space, though much more is needed.)

## Load Spatial Data

Let's first reload our spatial data -- this will be the MOUD points, plus the master zip code spatial file.

```{r warning = FALSE}
library(sf)
library(tmap)

points <- st_read("data/methadoneMOUD.geojson")
areas <- st_read("data/geo_export_54bc15d8-5ef5-40e4-8f72-bb0c6dbac9a5.shp")

head(points)
head(areas)

```

## Transform Projections

First we need to switch to a projection that uses distance in feet or meters as a metric. We'll use EPSG:3435 from the first tutorial. To find which EPSG was recommended, I searched "EPSG Illinois feet" and EPSG:3435 came up as a viable candidate. So, we use that for our new, projected CRS.

```{r}

areas <- st_transform(areas, 3435)
points <- st_transform(points, 3435)

```

We may want to once again confirm they are plotting correctly:

```{r}
tm_shape(areas) + tm_polygons() +
  tm_shape(points) + tm_dots(size = 1)
```

## Count resources by area

One way of understanding resource inequity is by thinking about how many resources exist in a neighborhood.

First, give the points the attributes of the polygons they are in. Inspect.

```{r}
pipr <- st_join(points, areas)
head(pipr)
```

You should have the same number of rows in `pipr` as you do in `points`. If not, there is something off. You may need to go back to troubleshoot. In an earlier version of this lab, I used a saved, written `geojson` file of the zip codes, and it would not render properly. Here, we load in the original shapefile at the beginning of the tutorial to avoid that error.

```{r}
dim(pipr)
dim(points)
dim(areas)
```

### Count # per Area
Next, count the number per area. The frequency should be logical according to the map you made earlier. Sometimes, I've found bugs where the numbers are multipled by some factor; this can be checked by looking at dimension disparities, as noted above.

```{r}
ptcount <- as.data.frame(table(pipr$Zip))
head(ptcount)
```
How could improve on this step if you used `dplyr`? 

**Aggregation Tip:** What if you have an attribute value you'd like to aggregate? For example, average units of affordable housing facility by zip? 
Try `aggregate(pip$attribute, by = list(pip$geoid), mean)` but build on with a tidy sensibility...

Now we can rename our attributes:

```{r}
names(ptcount) <- c("zip", "MetClnc")
head(ptcount)
```

And finally, merge back to your master zip file:

```{r}
head(areas)
areas<- merge(areas, ptcount, by="zip", all = TRUE)
head(areas)
```

Quickly map to inspect:
```{r}
tm_shape(areas) + tm_polygons(col = "gray80") +
tm_shape(areas) + tm_polygons(col = "MetClnc", style = "pretty", alpha = 0.8) +   
  tm_shape(points) + tm_dots(size = 0.5) 
```


## Buffer Data
Next, lets create a walkable buffer of one mile, or 5280 feet, for our MOUD provider locations. Individuals residing in places outside of that walkabile area may have difficulty accessing this medication during crises, like a pandemic.

```{r}
# Create 1mile buffers for each point
ptbuffers <- st_buffer(points, 5280)
```

Inspect immediately:

```{r}
tm_shape(areas) + tm_borders() +
  tm_shape(ptbuffers) + tm_borders(col = "blue") +
  tm_shape(points) + tm_dots(col = "red") 
```


## Count buffers by area
We know that MOUD locations are accessible up to one mile away. So, a total count of resources by area may be too restrictive. Let's calculate how many *walkable* MOUD clinics are in each zip code. Or, how many buffers are in each area...

```{r}
bufferct <- lengths(st_intersects(areas, ptbuffers))
head(bufferct)
```

Stick buffer totals back to the zip master file:

```{r}
# Stick buffer totals back to the census master file
areas <- cbind(areas,bufferct)
head(areas)
```

Map density of buffers per census area:

```{r}
tm_shape(areas) + tm_polygons(col = "bufferct", palette = "BuGn", n=5, style = "jenks") + 
    tm_shape(ptbuffers) + tm_fill(col = "gray90", alpha=0.2) +
    tm_shape(points) + tm_dots(col = "gray10", alpha = 0.8, size = 0.3) 
```

## Integrate & Explore

Let's review: our master area file now has total number resources by zip and total number of walkable service areas by zip. 

Using your new spatial file, see if you can answer some of these quetions using various queries:

- Which zip codes have high rates of COVID and are not within a walking distance of a methadone MOUD?

- Which zip codes have worse access to affordable rental units, low educational rates, and less walkable access to MOUDs? 

- What is the demographic and racial/ethnic characteristics of areas most vulnerable to high COVID rates in September 2020? 

Generate different maps and outputs to drive your thinking and defend your hypothesis formation.



<!--chapter:end:04-newsvar.Rmd-->

